{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from utils import printProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'pairs'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "input_files.sort()\n",
    "\n",
    "input_file_code = dict()\n",
    "input_file_code['converts'] = 'C'\n",
    "input_file_code['demonette1'] = 'D'\n",
    "input_file_code['mordan'] = 'M'\n",
    "\n",
    "input_file_code['denomCONVX'] = 'Na'\n",
    "input_file_code['denomPREFX'] = 'Nb'\n",
    "input_file_code['denomXaire'] = 'Nc'\n",
    "input_file_code['denomXal'] = 'Nd'\n",
    "input_file_code['denomXique'] = 'Ne'\n",
    "input_file_code['denomXSUF1'] = 'Nf'\n",
    "input_file_code['denomXSUF2'] = 'Ng'\n",
    "input_file_code['denomXSUF3'] = 'Nh'\n",
    "input_file_code['denomXSUF4'] = 'Ni'\n",
    "input_file_code['denomXSUF5'] = 'Nj'\n",
    "input_file_code['denomXSUF6'] = 'Nk'\n",
    "\n",
    "input_file_code['dimocXaie'] = 'Ma'\n",
    "input_file_code['dimocXat'] = 'Mb'\n",
    "input_file_code['dimocXet'] = 'Mc'\n",
    "input_file_code['dimocXier'] = 'Md'\n",
    "input_file_code['dimocXasmeXaste'] = 'Me'\n",
    "input_file_code['dimocXite'] = 'Mf'\n",
    "input_file_code['dimocXien'] = 'Mg'\n",
    "\n",
    "input_file_code['derifantiX'] = 'Ra'\n",
    "input_file_code['derifde1X'] = 'Rb'\n",
    "input_file_code['derifenX'] = 'Rc'\n",
    "input_file_code['derifinX'] = 'Rd'\n",
    "input_file_code['derifQUANTX'] = 'Re'\n",
    "input_file_code['derifreX'] = 'Rf'\n",
    "input_file_code['deriftriX'] = 'Rg'\n",
    "input_file_code['derifXable'] = 'Rh'\n",
    "input_file_code['derifXiser'] = 'Ri'\n",
    "\n",
    "# column number\n",
    "graph_1 = 3\n",
    "graph_2 = 6\n",
    "cat_1 = 8\n",
    "cat_2 = 10\n",
    "cstr_1 = 14\n",
    "cstr_2 = 17\n",
    "complexite = 19\n",
    "orientation = 21\n",
    "fichier_origine = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_representative(K):\n",
    "    # a representative of a family: the root (no direct relation going in), or the node with the largest outdegree\n",
    "    in_degree_dict = dict()  # only counting 'des2as' and 'as2des'\n",
    "    for v in list(K.nodes):\n",
    "        in_degree_dict[v] = 0\n",
    "    for e in list(K.edges.data()):\n",
    "        if not ('NA' in e[2]['label'] or 'indirect' in e[2]['label']):\n",
    "            in_degree_dict[e[1]] += 1\n",
    "    roots = list()\n",
    "    for k in in_degree_dict:\n",
    "        if in_degree_dict[k] == 0:\n",
    "            roots.append(k)\n",
    "    if len(roots) == 1:\n",
    "        return roots[0].split('_')[0]\n",
    "    elif len(roots) == 0:\n",
    "        max_out_degree = 0\n",
    "        selected_node = ''\n",
    "        for n in list(K.nodes):\n",
    "            if K.out_degree(n) > max_out_degree:\n",
    "                max_out_degree = K.out_degree(n)\n",
    "                selected_node = n\n",
    "        return selected_node.split('_')[0]\n",
    "    elif len(roots) > 1:\n",
    "        max_out_degree = 0\n",
    "        selected_root = roots[0]\n",
    "        for r in roots:\n",
    "            if K.out_degree(r) > max_out_degree:\n",
    "                max_out_degree = K.out_degree(r)\n",
    "                selected_root = r\n",
    "        return selected_root.split('_')[0]\n",
    "    \n",
    "def category_shortening(cat):\n",
    "    if cat != 'Num' and cat[0] == 'N':\n",
    "        if cat[1] == 'p':  # nom propre\n",
    "            return 'Np'\n",
    "        return 'N'  # nom\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ''\n",
    "G = nx.DiGraph()\n",
    "number_of_pairs = 0\n",
    "number_of_unique_pairs = 0\n",
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num == 0:\n",
    "                header = line.replace('\\n','')\n",
    "            elif line_num >= 2:\n",
    "                number_of_pairs += 1\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                v1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "                v2 = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if G.has_edge(v1, v2) or G.has_edge(v2, v1):\n",
    "                    continue\n",
    "                G.add_node(elements[graph_1] + '_' + elements[cat_1], label=category_shortening(elements[cat_1]))\n",
    "                G.add_node(elements[graph_2] + '_' + elements[cat_2], label=category_shortening(elements[cat_2]))\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    G.add_edge(v1, v2, label=elements[cstr_1] + '-' + elements[cstr_2]) # without complexity\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    G.add_edge(v2, v1, label=elements[cstr_2] + '-' + elements[cstr_1])\n",
    "                else:\n",
    "                    #sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    G.add_edge(v1, v2, label=elements[cstr_1] + '-' + elements[cstr_2] + '_' + elements[orientation])\n",
    "                    G.add_edge(v2, v1, label=elements[cstr_2] + '-' + elements[cstr_1] + '_' + elements[orientation])\n",
    "                number_of_unique_pairs += 1\n",
    "print(number_of_pairs, 'pairs')\n",
    "print(number_of_unique_pairs, 'unique pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_comps = list(nx.weakly_connected_components(G))\n",
    "number_of_families = len(conn_comps)\n",
    "print(number_of_families, 'families')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked = list()\n",
    "H = nx.Graph() # graph of graphs\n",
    "for fam1 in range(0, number_of_families):\n",
    "    if fam1 in checked:\n",
    "        continue\n",
    "    H.add_node(fam1)\n",
    "    for fam2 in range(fam1 + 1, number_of_families):\n",
    "        if fam2 in checked:\n",
    "            continue\n",
    "        G1 = nx.subgraph(G, conn_comps[fam1])\n",
    "        G2 = nx.subgraph(G, conn_comps[fam2])\n",
    "        if nx.is_isomorphic(G1, G2, node_match=lambda v1,v2: v1['label'] == v2['label'],\\\n",
    "                            edge_match=lambda e1,e2: e1['label'] == e2['label']):\n",
    "            checked.append(fam2)\n",
    "            H.add_edge(fam1, fam2)\n",
    "    printProgressBar(fam1 + 1, number_of_families, prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomorphy_graph = open('isomorphy_graph.p', 'wb')\n",
    "pickle.dump(H, isomorphy_graph)\n",
    "isomorphy_graph.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = pickle.load(open('isomorphy_graph.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H_conn_comps = list(nx.connected_components(H))\n",
    "H_conn_comps = [c for c in sorted(nx.connected_components(H), key=len, reverse=False)]\n",
    "print(len(H_conn_comps), 'groups')\n",
    "#H_conn_comps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'families'\n",
    "lexeme_dict = dict()\n",
    "f_summary = codecs.open('summary_of_families.txt', 'w+', encoding='utf-8')\n",
    "f_summary.write('family_id\\tnumber_of_lexemes\\tlexemes\\n')\n",
    "for fam_fam_id, fam_fam in enumerate(H_conn_comps):\n",
    "    for fam_id, fam in enumerate(fam_fam):\n",
    "        # first_member = sorted(list(conn_comps[fam]))[0]\n",
    "        representative = find_representative(nx.subgraph(G, conn_comps[fam]))\n",
    "        if len(fam_fam) == 1:\n",
    "            family_title = 'F' + str(fam_fam_id).rjust(5, '0')\n",
    "        else:\n",
    "            family_title = 'F' + str(fam_fam_id).rjust(5, '0') + '-' + str(fam_id).rjust(len(str(len(fam_fam)-1)), '0')\n",
    "        f_summary.write(family_title + '\\t'\\\n",
    "                       + str(len(conn_comps[fam])) + '\\t' + str(conn_comps[fam]) + '\\n')\n",
    "        for lexeme in conn_comps[fam]:\n",
    "            filename = family_title + ' ' + representative + '.txt'\n",
    "            lexeme_dict[lexeme] = filename\n",
    "            f_out = codecs.open(join(output_folder, filename), 'w+', encoding='utf-8')\n",
    "            f_out.write(header + '\\tfichier_origine' + '\\n\\n')\n",
    "            f_out.close()\n",
    "f_summary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num < 2:\n",
    "                continue\n",
    "            elements = line.replace(' ','').split('\\t')\n",
    "            lexeme1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "            if elements[complexite] not in ['simple', 'complexe', 'motiv-form', 'motiv-sem', 'accidentel']:\n",
    "                print('warning ', input_file)\n",
    "            output_filename = lexeme_dict[lexeme1]\n",
    "            f_out = codecs.open(join(output_folder, output_filename), 'a+', encoding='utf-8')\n",
    "            f_out.write(line.strip('\\n') + '\\t' + input_file_code.get(input_file.split('-')[0]) + '\\n')\n",
    "            f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get all categories, lexemes, orientation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ''\n",
    "list_of_categories = set()\n",
    "lexemes_with_nonalpha = set()\n",
    "input_dir = 'pairs'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num == 0:\n",
    "                header = line.replace('\\n','')\n",
    "            elif line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                if elements[cat_2] == 'More':\n",
    "                    print(elements[graph_2])\n",
    "                list_of_categories.add(elements[cat_1])\n",
    "                list_of_categories.add(elements[cat_2])\n",
    "                if not elements[graph_1].isalpha():\n",
    "                    lexemes_with_nonalpha.add(elements[graph_1])\n",
    "                if not elements[graph_2].isalpha():\n",
    "                    lexemes_with_nonalpha.add(elements[graph_2])\n",
    "print('categories: ' + str(list_of_categories))\n",
    "#print(lexemes_with_nonalpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of double arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_direction(input_orientation):\n",
    "    if input_orientation == 'de2as':\n",
    "        input_orientation = 'as2de'\n",
    "    elif input_orientation == 'as2de':\n",
    "        input_orientation = 'de2as'\n",
    "    return input_orientation\n",
    "\n",
    "\n",
    "def line_writer(input_line):\n",
    "    elements = line.replace('\\n','').replace(' ','').replace('des2as', 'de2as').replace('as2des', 'as2de').split('\\t')\n",
    "    ret_line = elements[graph_1] + '\\t' + elements[cat_1] + '\\t' + elements[graph_2] + '\\t' + elements[cat_2] + '\\t'\\\n",
    "    + elements[orientation] + '\\t' + elements[complexite] + '\\t' + elements[cstr_1] + '\\t' + elements[cstr_2] + '\\t'\\\n",
    "    + elements[fichier_origine] + '\\n'\n",
    "    return ret_line\n",
    "\n",
    "\n",
    "f_duplicates = codecs.open('paires_doublons.txt', 'w+', encoding='utf-8')\n",
    "f_duplicates.write('graph_1\\tcat_1\\tgraph_2\\tcat_2\\torientation\\tcomplexite\\tcstr_1\\tcstr_2\\tfichier_origine\\n')\n",
    "input_dir = 'families'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "counter = 0\n",
    "for input_file in input_files:\n",
    "    G = nx.Graph()\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                reverse = False\n",
    "                elements = line.replace('\\n','').replace(' ','').replace('des2as', 'de2as').replace('as2des', 'as2de').split('\\t')\n",
    "                v1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "                v2 = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if v1 > v2:\n",
    "                    reverse = True\n",
    "                if reverse:\n",
    "                    edge_label = elements[complexite] + '_' + reverse_direction(elements[orientation]) + '_'\\\n",
    "                    + elements[cstr_2] + '-' + elements[cstr_1]\n",
    "                else:\n",
    "                    edge_label = elements[complexite] + '_' + elements[orientation] + '_' \\\n",
    "                    + elements[cstr_1] + '-' + elements[cstr_2]\n",
    "                if G.has_edge(v1, v2) and G[v1][v2]['label'] != edge_label and not G[v1][v2]['checked']:\n",
    "                    f_duplicates.write(G[v1][v2]['complete_line'])\n",
    "                    f_duplicates.write(line_writer(line) + '\\n')\n",
    "                    G[v1][v2]['checked'] = True\n",
    "                elif not G.has_edge(v1, v2):\n",
    "                    G.add_edge(v1, v2, label=edge_label, complete_line=line_writer(line), checked=False)\n",
    "    counter += 1\n",
    "    printProgressBar(counter + 1, len(input_files), prefix='Progress:', suffix='complete', length=50, decimals=2)\n",
    "f_duplicates.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fingerprints\n",
    "Category is used when creating nodes (to avoid joining two lexemes with same graphie but different category), but not for comparing isomorphy.\n",
    "\n",
    "Ignoring pairs with \"indirect\" and \"NA\" as orientation -> possible for some lexemes to be isolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ''\n",
    "G = nx.DiGraph()\n",
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num == 0:\n",
    "                header = line.replace('\\n','')\n",
    "            elif line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                G.add_node(elements[graph_1] + '_' + elements[cat_1], label='')\n",
    "                G.add_node(elements[graph_2] + '_' + elements[cat_2], label='')\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    edge_type = elements[cstr_1] + '-' + elements[cstr_2]\n",
    "                    G.add_edge(elements[graph_1] + '_' + elements[cat_1], elements[graph_2] + '_' + elements[cat_2],\\\n",
    "                               label=edge_type)\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    edge_type = elements[cstr_2] + '-' + elements[cstr_1]\n",
    "                    G.add_edge(elements[graph_2] + '_' + elements[cat_2], elements[graph_1] + '_' + elements[cat_1],\\\n",
    "                               label=edge_type)\n",
    "                elif elements[orientation] == 'NA':\n",
    "                    edge_type = elements[cstr_1] + '-' + elements[cstr_2]\n",
    "                    G.add_edge(elements[graph_1] + '_' + elements[cat_1], elements[graph_2] + '_' + elements[cat_2],\\\n",
    "                               label=edge_type)\n",
    "                    edge_type = elements[cstr_2] + '-' + elements[cstr_1]\n",
    "                    G.add_edge(elements[graph_2] + '_' + elements[cat_2], elements[graph_1] + '_' + elements[cat_1],\\\n",
    "                               label=edge_type)\n",
    "                else:  # orientation: indirect\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_comps = list(nx.weakly_connected_components(G))\n",
    "number_of_families = len(conn_comps)\n",
    "number_of_families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked = list()\n",
    "H = nx.Graph() # graph of graphs\n",
    "for fam1 in range(0, number_of_families):\n",
    "    if fam1 in checked:\n",
    "        continue\n",
    "    H.add_node(fam1)\n",
    "    G1 = nx.subgraph(G, conn_comps[fam1])\n",
    "    if len(G1) == 1:  # isolated\n",
    "        continue\n",
    "    for fam2 in range(fam1 + 1, number_of_families):\n",
    "        if fam2 in checked:\n",
    "            continue\n",
    "        G2 = nx.subgraph(G, conn_comps[fam2])\n",
    "        if len(G2) == 1:  # isolated\n",
    "            continue\n",
    "        if nx.is_isomorphic(G1, G2, node_match=lambda v1,v2: v1['label'] == v2['label'],\\\n",
    "                            edge_match=lambda e1,e2: e1['label'] == e2['label']):\n",
    "            checked.append(fam2)\n",
    "            H.add_edge(fam1, fam2)\n",
    "    printProgressBar(fam1 + 1, number_of_families, prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomorphy_fingerprint = open('isomorphy_fingerprint.p', 'wb')\n",
    "pickle.dump(H, isomorphy_fingerprint)\n",
    "isomorphy_fingerprint.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = pickle.load(open('isomorphy_fingerprint.p', 'rb'))\n",
    "#H_conn_comps = list(nx.connected_components(H))\n",
    "H_conn_comps = [c for c in sorted(nx.connected_components(H), key=len, reverse=False)]\n",
    "print(len(H_conn_comps))\n",
    "#H_conn_comps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'families_fingerprint'\n",
    "lexeme_dict = dict()\n",
    "f_summary = codecs.open('summary_of_families_from_fingerprint.txt', 'w+', encoding='utf-8')\n",
    "f_summary.write('family_id\\tnumber_of_lexemes\\tlexemes\\n')\n",
    "for fam_fam_id, fam_fam in enumerate(H_conn_comps):\n",
    "    for fam_id, fam in enumerate(fam_fam):\n",
    "        # first_member = sorted(list(conn_comps[fam]))[0]\n",
    "        representative = find_representative(nx.subgraph(G, conn_comps[fam]))\n",
    "        if len(fam_fam) == 1:\n",
    "            family_title = 'F' + str(fam_fam_id).rjust(5, '0')\n",
    "        else:\n",
    "            family_title = 'F' + str(fam_fam_id).rjust(5, '0') + '-' + str(fam_id).rjust(len(str(len(fam_fam)-1)), '0')\n",
    "        f_summary.write(family_title + '\\t'\\\n",
    "                       + str(len(conn_comps[fam])) + '\\t' + str(conn_comps[fam]) + '\\n')\n",
    "        for lexeme in conn_comps[fam]:\n",
    "            filename = family_title + ' ' + representative + '.txt'\n",
    "            lexeme_dict[lexeme] = filename\n",
    "            f_out = codecs.open(join(output_folder, filename), 'w+', encoding='utf-8')\n",
    "            f_out.write(header + '\\tfichier_origine' + '\\n\\n')\n",
    "            f_out.close()\n",
    "f_summary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num < 2:\n",
    "                continue\n",
    "            elements = line.replace(' ','').split('\\t')\n",
    "            lexeme1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "            if elements[complexite] not in ['simple', 'complexe', 'motiv-form', 'motiv-sem', 'accidentel']:\n",
    "                print('warning ', input_file)\n",
    "            output_filename = lexeme_dict[lexeme1]\n",
    "            f_out = codecs.open(join(output_folder, output_filename), 'a+', encoding='utf-8')\n",
    "            f_out.write(line.strip('\\n') + '\\t' + input_files[input_file] + '\\n')\n",
    "            f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# homonyms Nm Nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonde Nf F00209 Nm F00210\n",
      "philosophe Nm F01320 Nf F01321\n",
      "philosophe Nm F01320 Nf F01321\n",
      "philosophe Nm F01320 Nf F01321\n",
      "philosophe Nm F01320 Nf F01321\n",
      "philosophe Nm F01320 Nf F01321\n",
      "philosophe Nm F01320 Nf F01321\n",
      "somme Nf F01597 Nm F01598\n",
      "somme Nf F01597 Nm F01598\n",
      "vis Nm F01508 Nf F01775\n",
      "vis Nm F01508 Nf F01775\n",
      "vis Nm F01508 Nf F01775\n",
      "vis Nm F01508 Nf F01775\n",
      "vis Nm F01508 Nf F01775\n",
      "vis Nm F01508 Nf F01775\n",
      "carbonite Nf F00358 Nm F02038\n",
      "masque Nm F01112 Nf F02271\n",
      "masque Nm F01112 Nf F02271\n",
      "émissaire Nf F00717 Nm F02574\n",
      "modèle Nm F01153 Nf F02885\n",
      "modèle Nm F01153 Nf F02885\n",
      "modèle Nm F01153 Nf F02885\n",
      "chlorite Nm F00384 Nf F03127\n",
      "chlorite Nm F00384 Nf F03127\n",
      "coche Nf F00415 Nm F03449\n",
      "coche Nf F00415 Nm F03449\n",
      "coche Nf F00415 Nm F03449\n",
      "pirate Nm F01343 Nf F03471\n",
      "pirate Nm F01343 Nf F03471\n",
      "pirate Nm F01343 Nf F03471\n",
      "pirate Nm F01343 Nf F03471\n",
      "Terre Nf F01667 Npx F03547\n",
      "Terre Nf F01667 Npx F03547\n",
      "Terre Nf F01667 Npx F03547\n",
      "bleueur Nm F01997 Nf F03594\n",
      "Sahara Nm F03638 Npx F03644\n",
      "Sahara Nm F03638 Npx F03644\n",
      "Sahara Nm F03638 Npx F03644\n",
      "Sahara Nm F03638 Npx F03644\n",
      "Pâques Nf F01240 Npx F03803\n",
      "Pâques Nf F01240 Npx F03803\n",
      "Pâques Nf F01240 Npx F03803\n",
      "Pâques Nf F01240 Npx F03803\n",
      "serpent Nm F01580 Nf F03811\n",
      "serpent Nm F01580 Nf F03811\n",
      "météorite Nm F02875 Nf F03932\n",
      "météorite Nm F02875 Nf F03932\n",
      "os Nf F02364 Nm F03953\n",
      "os Nf F02364 Nm F03953\n",
      "os Nf F02364 Nm F03953\n",
      "os Nf F02364 Nm F03953\n",
      "os Nf F02364 Nm F03953\n",
      "os Nf F02364 Nm F03953\n",
      "saveur Nm F01560 Nf F04180\n",
      "amour Nf F02502 Nm F04254\n",
      "amour Nf F02502 Nm F04254\n",
      "amour Nf F02502 Nm F04254\n",
      "vivre Nm F02402 Nf F04302\n",
      "vivre Nm F02402 Nf F04302\n",
      "Jacques Npm F02804 Npx F04344\n",
      "Jacques Npm F02804 Npx F04344\n",
      "oreille Nf F03954 Nm F04422\n",
      "oreille Nf F03954 Nm F04422\n",
      "oreille Nf F03954 Nm F04422\n",
      "archiviste Nf F00095 Nm F04521\n",
      "artiste Nf F03487 Nm F04522\n",
      "artiste Nf F03487 Nm F04522\n",
      "fonctionnaire Nf F00825 Nm F04545\n",
      "fonctionnaire Nf F00825 Nm F04545\n",
      "fonctionnaire Nf F00825 Nm F04545\n",
      "fonctionnaire Nf F00825 Nm F04545\n",
      "fonctionnaire Nf F00825 Nm F04545\n",
      "jeune Nf F03495 Nm F04554\n",
      "jeune Nf F03495 Nm F04554\n",
      "jeune Nf F03495 Nm F04554\n",
      "pote Nf F03330 Nm F04573\n",
      "pote Nf F03330 Nm F04573\n",
      "pote Nf F03330 Nm F04573\n",
      "vampire Nf F03298 Nm F04590\n",
      "vampire Nf F03298 Nm F04590\n",
      "vampire Nf F03298 Nm F04590\n",
      "vénérable Nf F03306 Nm F04592\n",
      "vénérable Nf F03306 Nm F04592\n",
      "vénérable Nf F03306 Nm F04592\n",
      "tarte Nf F02375 Nm F04601\n",
      "cancer Nf F02028 Nm F04632\n",
      "cancer Nf F02028 Nm F04632\n",
      "livre Nf F04143 Nm F04666\n",
      "livre Nf F04143 Nm F04666\n",
      "livre Nf F04143 Nm F04666\n",
      "bonhomme Nf F03419 Nm F04687\n",
      "bonhomme Nf F03419 Nm F04687\n",
      "Robin Npx F04464 Npm F04740\n",
      "Laurent Npm F04654 Npx F04793\n",
      "Laurent Npm F04654 Npx F04793\n",
      "Marc Npm F04705 Npx F05029\n",
      "Marc Npm F04705 Npx F05029\n",
      "jaque Nf F04344 Nm F05041\n",
      "jaque Nf F04344 Nm F05041\n",
      "jaque Nf F04344 Nm F05041\n",
      "baume Nf F02481 Nm F05051\n",
      "baume Nf F02481 Nm F05051\n",
      "baume Nf F02481 Nm F05051\n",
      "baume Nf F02481 Nm F05051\n",
      "feutre Nm F00789 Npx F05058\n",
      "alimentation Nf F00051 Nm F05073\n",
      "anagramme Nf F03832 Nm F05157-1\n",
      "anagramme Nf F03832 Nm F05157-1\n",
      "snob Nm F01592 Nf F05377-0\n",
      "manutentionnaire Nf F01098 Nm F05424-1\n",
      "élite Nf F02476 Nm F05425-1\n",
      "élite Nf F02476 Nm F05425-1\n",
      "élite Nf F02476 Nm F05425-1\n",
      "élite Nf F02476 Nm F05425-1\n",
      "olive Nf F01223 Nm F05442-0\n",
      "caque Nf F00302 Nm F05442-1\n",
      "Samson Npm F05452-0 Npx F05501-1\n",
      "Samson Npm F05452-0 Npx F05501-1\n",
      "Adam Npm F04726 Npx F05577-1\n",
      "Adam Npm F04726 Npx F05577-1\n",
      "Adam Npm F04726 Npx F05577-1\n",
      "nummulaire Nf F01155 Nm F05625-2\n",
      "Marie Npf F03927 Npx F05666-3\n",
      "scorpion Nm F04651 Nf F05673-3\n",
      "scorpion Nm F04651 Nf F05673-3\n",
      "aigle Nm F04619 Nf F05677-0\n",
      "lion Nm F02476 Nf F05677-3\n",
      "évêque Nm F04253 Nf F05682-1\n",
      "expert Nf F00764 Nm F05683-1\n",
      "architecte Nf F00094 Nm F05683-2\n",
      "esclave Nf F03437 Nm F05745-0\n",
      "esclave Nf F03437 Nm F05745-0\n",
      "Martin Npm F05452-1 Npx F05751-3\n",
      "Martin Npm F05452-1 Npx F05751-3\n",
      "internaute Nm F05425-0 Nf F05764-4\n",
      "internaute Nm F05425-0 Nf F05764-4\n",
      "elfe Nf F03883 Nm F05776-2\n",
      "basket Nm F05545-0 Nf F05776-4\n",
      "pendule Nf F03706 Nm F05808-2\n",
      "notable Nf F01207 Nm F05822-1\n",
      "notable Nf F01207 Nm F05822-1\n",
      "kamikaze Nf F05325-0 Nm F05822-4\n",
      "kamikaze Nf F05325-0 Nm F05822-4\n",
      "khédive Nf F05730-2 Nm F05822-5\n",
      "khédive Nf F05730-2 Nm F05822-5\n",
      "champagne Nf F02047 Nm F05825-4\n",
      "champagne Nf F02047 Nm F05825-4\n",
      "volontaire Nm F03577 Nf F05836-5\n",
      "parlementaire Nm F01273 Nf F05836-6\n",
      "Sarkozy Npx F04612 Npm F05844-3\n",
      "Hollande Npx F05371-1 Npm F05844-5\n",
      "Jospin Npx F05798-1 Npm F05844-7\n",
      "slave Nm F03188 Nf F05854-2\n",
      "sybarite Nm F05405-1 Nf F05855-9\n",
      "éloge Nf F05842-3 Nm F05869-01\n",
      "aune Nm F05604-2 Nf F05887-08\n",
      "célibataire Nm F03673 Nf F05894-10\n",
      "phlogistique Nf F04413 Nm F05894-11\n",
      "tank Nm F03237 Nf F05894-12\n",
      "Merlin Npx F04717 Npm F05917-04\n",
      "Berthe Npx F04746 Npf F05917-11\n",
      "dindon Nm F00618 Nf F05926-00\n",
      "mille Num F03401 Nm F05929-08\n",
      "mille Num F03401 Nm F05929-08\n",
      "mille Num F03401 Nm F05929-08\n",
      "robot Nm F01515 Nf F05932-21\n",
      "Madeleine Npx F04239 Npf F05936-13\n",
      "bau Nf F00678 Nm F05953-09\n",
      "tangue Nm F05773-2 Nf F05953-12\n",
      "taureau Nf F05677-2 Nm F05953-15\n",
      "arène Nm F01925 Nf F05964-04\n",
      "arénaire Nf F01537 Nm F05964-04\n",
      "clavaire Nm F04105 Nf F05964-07\n",
      "bourre Nf F00229 Nm F05972-10\n",
      "somnambule Nf F03995 Nm F05981-51\n",
      "toiture Nf F04633 Nm F05981-71\n",
      "prosélyte Nf F05855-0 Nm F05984-00\n",
      "jade Nm F05818-5 Nf F05984-50\n",
      "pilote Nm F01335 Nf F05984-55\n",
      "ébène Nm F05979-62 Nf F05984-61\n",
      "clone Nf F03615 Nm F05985-25\n",
      "gîte Nf F05820-3 Nm F05985-54\n",
      "bourbe Nf F02186 Nm F05987-003\n",
      "plante Nf F00990 Nm F05987-008\n",
      "troche Nf F05972-39 Nm F05987-021\n",
      "ranche Nf F05623-0 Nm F05987-030\n",
      "aile Nf F00044 Nm F05987-034\n",
      "dartre Nf F05387-1 Nm F05987-045\n",
      "scrofule Nm F05964-31 Nf F05988-055\n",
      "guide Nm F00946 Nf F05988-086\n",
      "novice Nm F05822-2 Nf F05990-061\n",
      "catéchumène Nm F05924-13 Nf F05990-146\n",
      "Zola Npx F04747 Npm F05991-002\n",
      "Victoria Npx F03624 Npf F05991-006\n",
      "Sonora Npf F05801-0 Npx F05991-007\n",
      "gazon Nm F00888 Npx F05991-029\n",
      "Salomé Npx F05986-88 Npf F05991-043\n",
      "France Npx F02647 Npf F05991-104\n",
      "Adonis Npx F01842 Npm F05991-108\n",
      "Henri Npx F05967-44 Npm F05991-121\n",
      "Maurice Npx F04278 Npm F05991-130\n",
      "Charles Npx F03386 Npm F05991-134\n",
      "foudre Nf F05818-2 Nm F05992-099\n",
      "foudre Nf F05818-2 Nm F05992-099\n",
      "interligne Nm F05525-2 Nf F05993-037\n",
      "méduse Npx F01123 Nf F05993-041\n",
      "homicide Nm F00972 Nf F05993-075\n",
      "énarque Nf F03886 Nm F05994-081\n",
      "actuaire Nf F00024 Nm F05997-021\n",
      "barbouze Nf F03440 Nm F05997-038\n",
      "bolchevik Nf F01999 Nm F05997-042\n",
      "brahmane Nf F03408 Nm F05997-044\n",
      "convive Nf F03775 Nm F05997-070\n",
      "dépositaire Nf F00587 Nm F05997-079\n",
      "despote Nf F03875 Nm F05997-081\n",
      "destinataire Nf F00603 Nm F05997-082\n",
      "exorciste Nf F02597 Nm F05997-099\n",
      "interne Nf F05590-2 Nm F05997-135\n",
      "libraire Nf F04666 Nm F05997-144\n",
      "locataire Nf F01076 Nm F05997-145\n",
      "mandataire Nf F01094 Nm F05997-150\n",
      "registraire Nf F01473 Nm F05997-196\n",
      "tapir Nf F05950-24 Nm F05997-231\n",
      "Tremblay Nm F01703 Npx F05998-099\n",
      "Tremblay Nm F01703 Npx F05998-099\n",
      "Joseph Npm F05991-128 Npx F05998-108\n",
      "Joseph Npm F05991-128 Npx F05998-108\n",
      "Virginie Npf F05991-059 Npx F05998-141\n",
      "Virginie Npf F05991-059 Npx F05998-141\n",
      "Virginie Npf F05991-059 Npx F05998-141\n",
      "Aube Npf F05991-011 Npx F05998-188\n",
      "Aube Npf F05991-011 Npx F05998-188\n",
      "Guillaume Npm F00947 Npx F05998-202\n",
      "Guillaume Npm F00947 Npx F05998-202\n",
      "vamp Nf F05687-3 Nm F05999-029\n",
      "matamore Nf F05954-03 Nm F05999-055\n",
      "fécule Nf F04300 Nm F05999-056\n",
      "matricule Nf F05993-094 Nm F05999-058\n",
      "arsouille Nf F05984-06 Nm F05999-069\n",
      "rêne Nf F02525 Nm F05999-071\n",
      "cosmétique Nf F05965-12 Nm F05999-099\n",
      "froc Nf F00565 Nm F05999-103\n",
      "froc Nf F00565 Nm F05999-103\n",
      "froc Nf F00565 Nm F05999-103\n",
      "second Num F03717 Nm F05999-128\n",
      "cagne Nf F04071 Nm F05999-186\n",
      "maille Nf F01087 Nm F06002-001\n",
      "feuilleton Nf F05384-0 Nm F06002-024\n",
      "fan Nm F02619 Nf F06002-028\n",
      "banne Nm F00156 Nf F06002-073\n",
      "bogue Nm F05999-143 Nf F06002-108\n",
      "lac Nf F04387 Nm F06002-135\n",
      "boum Nm F05981-52 Nf F06002-154\n",
      "fanzine Nf F05427-1 Nm F06002-176\n",
      "camarade Nm F05941-12 Nf F06002-192\n",
      "supermarché Nf F06002-124 Nm F06002-235\n",
      "senior Nf F03776 Nm F06002-236\n",
      "pamplemousse Nf F05821-2 Nm F06002-244\n",
      "tome Nm F05999-213 Nf F06002-258\n",
      "Tonton Npm F05844-1 Npx F06004-213\n",
      "Tonton Npm F05844-1 Npx F06004-213\n",
      "Tonton Npm F05844-1 Npx F06004-213\n",
      "Tonton Npm F05844-1 Npx F06004-213\n",
      "pédagogue Nm F05999-177 Nf F06006-318\n",
      "Charlotte Npf F05801-6 Npx F06009-055\n",
      "Charlotte Npf F05801-6 Npx F06009-055\n",
      "Yves Npm F05991-133 Npx F06009-792\n",
      "Yves Npm F05991-133 Npx F06009-792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazz Nm F05935-05 Nf F06010-0478\n",
      "lévite Nm F04556 Nf F06010-0496\n",
      "mime Nm F05999-075 Nf F06010-0571\n",
      "mythe Nf F02286 Nm F06010-0602\n",
      "oestrogène Nm F05792-0 Nx F06010-0645\n",
      "quartzite Nm F05354-1 Nf F06010-0850\n",
      "silicone Nm F05948-13 Nf F06010-0911\n",
      "systole Nm F05993-060 Nf F06010-0971\n",
      "troglodyte Nm F05984-15 Nf F06010-1041\n",
      "xérophyte Nm F05969-11 Nf F06010-1068\n",
      "acétylène Nm F00021 Nf F06010-1087\n",
      "anastomose Nm F05999-049 Nf F06010-1158\n",
      "Arthur Npm F05991-106 Npx F06011-0031\n",
      "Arthur Npm F05991-106 Npx F06011-0031\n",
      "Arthur Npm F05991-106 Npx F06011-0031\n",
      "Céline Npf F05991-084 Npx F06011-0052\n",
      "Céline Npf F05991-084 Npx F06011-0052\n",
      "Céline Npf F05991-084 Npx F06011-0052\n",
      "Attila Npm F05991-009 Npx F06011-0223\n",
      "Attila Npm F05991-009 Npx F06011-0223\n",
      "Catherine Npf F05936-15 Npx F06011-0378\n",
      "Catherine Npf F05936-15 Npx F06011-0378\n",
      "PAF Nm F06002-164 Npx F06011-0538\n",
      "PAF Nm F06002-164 Npx F06011-0538\n",
      "Robinson Npm F05991-097 Npx F06011-0604\n",
      "Robinson Npm F05991-097 Npx F06011-0604\n"
     ]
    }
   ],
   "source": [
    "word_cat = dict()\n",
    "input_dir = 'families'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "input_files.sort()\n",
    "count = 0\n",
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num < 2:\n",
    "                continue\n",
    "            elements = line.replace(' ','').split('\\t')\n",
    "            lexeme = elements[graph_1]\n",
    "            cat = word_cat.get(lexeme)\n",
    "            if cat != None and cat[0] == 'N':\n",
    "                if cat != elements[cat_1] and elements[cat_1][0] == 'N' and cat.split()[1] != input_file.split()[0]:\n",
    "                    print(lexeme, cat, elements[cat_1], input_file.split()[0])\n",
    "                    count += 1\n",
    "            else:\n",
    "                word_cat[lexeme] = elements[cat_1] + ' ' + input_file.split()[0]\n",
    "            lexeme = elements[graph_2]\n",
    "            cat = word_cat.get(lexeme)\n",
    "            if cat != None and cat[0] == 'N':\n",
    "                if cat != elements[cat_2] and elements[cat_2][0] == 'N' and cat.split()[1] != input_file.split()[0]:\n",
    "                    print(lexeme, cat, elements[cat_2], input_file.split()[0])\n",
    "                    count += 1\n",
    "            else:\n",
    "                word_cat[lexeme] = elements[cat_2] + ' ' + input_file.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
