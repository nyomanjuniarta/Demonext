{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import networkx as nx\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "#from numpy.linalg import norm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from utils import printProgressBar\n",
    "\n",
    "# column number\n",
    "graph_1 = 3\n",
    "graph_2 = 6\n",
    "cat_1 = 8\n",
    "cat_2 = 10\n",
    "cstr_1 = 14\n",
    "cstr_2 = 17\n",
    "complexite = 19\n",
    "orientation = 21\n",
    "fichier_origine = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract all valid cstr (Xeur, Xette, reX, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dir = 'families'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "valid_cstr = set()\n",
    "counter = 0\n",
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                elements = line.replace(' ', '').split('\\t')\n",
    "                valid_cstr.add(elements[cstr_1].replace('1', '').replace('2', ''))\n",
    "                valid_cstr.add(elements[cstr_2].replace('1', '').replace('2', ''))\n",
    "    counter += 1\n",
    "    printProgressBar(counter, len(input_files), prefix='Progress:', suffix='complete', length=50, decimals=2)\n",
    "valid_cstr.update(['dX', 'rX', 'réX'])\n",
    "print(sorted(list(valid_cstr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "family_dict = dict()\n",
    "with codecs.open('summary_of_families.txt', 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f):\n",
    "        if line_num == 0:\n",
    "            continue\n",
    "        cols = line.split('\\t')\n",
    "        lexemes = cols[2].replace('{','').replace('}','').replace('\\'','').split(', ')\n",
    "        lexeme_set = set()\n",
    "        for lexeme in lexemes:\n",
    "            lexeme_set.add(lexeme.split('_')[0])\n",
    "        family_dict[cols[0]] = lexeme_set\n",
    "print(len(family_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "family_dict_keys = list(family_dict.keys())\n",
    "min_proportion = 0.5\n",
    "df = pd.DataFrame(columns=['family_id_1', 'family_id_2', 'suspected', 'construction'])\n",
    "for k1 in range(0, len(family_dict)):\n",
    "    for k2 in range(k1+1, len(family_dict)):\n",
    "        set1 = family_dict[family_dict_keys[k1]]\n",
    "        set2 = family_dict[family_dict_keys[k2]]\n",
    "        connected = False\n",
    "        suspected = ''\n",
    "        construction = ''\n",
    "        for s1 in set1:\n",
    "            for s2 in set2:\n",
    "                if s1 in s2 and s2.replace(s1, 'X') in valid_cstr:\n",
    "                    construction = s2.replace(s1, 'X')\n",
    "                    connected = True\n",
    "                    break\n",
    "                elif s2 in s1 and s1.replace(s2, 'X') in valid_cstr:\n",
    "                    construction = s1.replace(s2, 'X')\n",
    "                    connected = True\n",
    "                    break\n",
    "            if connected:\n",
    "                break\n",
    "        if connected:\n",
    "            df = df.append(pd.Series({\n",
    "                'family_id_1': family_dict_keys[k1],\n",
    "                'family_id_2': family_dict_keys[k2],\n",
    "                'suspected': s1 + '-' + s2,\n",
    "                'construction': construction\n",
    "            }), ignore_index=True)\n",
    "    printProgressBar(k1 + 1, len(family_dict), prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)\n",
    "df.to_excel('maybe_connected_cstr.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# AOC-poset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_with_header = pd.read_csv('context_from_maxgraph.csv', header=0, index_col=0)\n",
    "family_ids = context_with_header.index\n",
    "\n",
    "family_dict = dict()\n",
    "with codecs.open('summary_of_families.txt', 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f):\n",
    "        if line_num == 0:\n",
    "            continue\n",
    "        cols = line.split('\\t')\n",
    "        lexemes = cols[2].replace('{','').replace('}','').replace('\\'','').split(', ')\n",
    "        lexeme_set = set()\n",
    "        for lexeme in lexemes:\n",
    "            lexeme_set.add(lexeme.split('_')[0])\n",
    "        family_dict[cols[0]] = lexeme_set\n",
    "print(len(family_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = nx.DiGraph()\n",
    "vertex_graph_dict = dict()\n",
    "with codecs.open(join('posets', 'families_simplified_maxgraph.dot'), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if '->' in line:  # a line showing edges between concepts\n",
    "            elements = line.split()\n",
    "            L.add_edge(vertex_graph_dict[elements[2]], vertex_graph_dict[elements[0]])\n",
    "        elif 'shape' in line:  # a line describing a concept \n",
    "            vertex_number = line.split()[0]\n",
    "            graph_number = re.search('Attribute (.*)\\|', line.replace('\\\\n', '')).group(1)\n",
    "            object_string = line.split('|')[2]\n",
    "            objects = object_string.split('\\\\n')\n",
    "            family_id_set = set()\n",
    "            for obj in objects:\n",
    "                if obj == '' or '}' in obj:\n",
    "                    continue\n",
    "                family_id_set.add(family_ids[int(obj.split()[1])])\n",
    "            L.add_node(graph_number, families=family_id_set)\n",
    "            vertex_graph_dict[vertex_number] = graph_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L.nodes['3800']['families']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "family_dict['F03800-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_of_maxgraphs = 3903\n",
    "temp = codecs.open('temp.txt', 'w')\n",
    "counter = 0\n",
    "teetotal = 0\n",
    "for g1 in range(0, number_of_maxgraphs):\n",
    "    for g2 in range(g1 + 1, number_of_maxgraphs):\n",
    "        if not (str(g1) in nx.descendants(L, str(g2)) or str(g2) in nx.descendants(L, str(g1))):\n",
    "            # if a concept is not a descendant of another\n",
    "            # temp.write(str(g1) + ' ' + str(g2) + '\\n')\n",
    "            teetotal += 1\n",
    "        counter += 1\n",
    "        printProgressBar(counter, number_of_maxgraphs * number_of_maxgraphs / 2, prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)\n",
    "temp.close()\n",
    "print(teetotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin', binary=True, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'approchement' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ee3f4b1e9f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'approchement'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\user\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\user\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\user\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'approchement' not present\""
     ]
    }
   ],
   "source": [
    "model.most_similar('approchement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009172685"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('boucher', 'déboucher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
