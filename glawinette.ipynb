{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb81d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from networkx.drawing.nx_pydot import write_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column number\n",
    "lemma1 = 0\n",
    "lemma2 = 1\n",
    "cat1 = 2\n",
    "cat2 = 3\n",
    "origine_morpho = 4\n",
    "origine_def = 5\n",
    "BAP1 = 6\n",
    "BAP2 = 7\n",
    "BAPsize = 8\n",
    "FAP1 = 9\n",
    "FAP2 = 10\n",
    "FAPsize = 11\n",
    "radical = 12\n",
    "FAPtype = 13\n",
    "\n",
    "def FAPconverter(input_fap):\n",
    "    return input_fap.replace('(.+)', 'X').replace('$', '').replace('^', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ''\n",
    "G = nx.Graph()\n",
    "number_of_pairs = 0\n",
    "number_of_unique_pairs = 0\n",
    "with codecs.open('glawinette-series.csv', 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f):\n",
    "        if line_num == 0:\n",
    "            header = line.replace('\\n','')\n",
    "        elif line_num >= 1:\n",
    "            elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "            v1 = elements[lemma1] + '_' + elements[cat1]\n",
    "            v2 = elements[lemma2] + '_' + elements[cat2]\n",
    "            if G.has_edge(v1, v2):\n",
    "                continue\n",
    "            number_of_pairs += 1\n",
    "            G.add_node(v1, label=elements[cat1])\n",
    "            G.add_node(v2, label=elements[cat1])\n",
    "            pattern1 = FAPconverter(elements[FAP1])\n",
    "            pattern2 = FAPconverter(elements[FAP2])\n",
    "            sorted_pattern = sorted([pattern1, pattern2])\n",
    "            G.add_edge(v1, v2, label=sorted_pattern[0] + '-' + sorted_pattern[1])\n",
    "print(number_of_pairs, 'pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a41512",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_comps = list(nx.connected_components(G))\n",
    "number_of_families = len(conn_comps)\n",
    "print(number_of_families, 'families')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68781d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checked = list()\n",
    "H = nx.Graph() # graph of graphs\n",
    "for fam1 in range(0, number_of_families):\n",
    "    if fam1 in checked:\n",
    "        continue\n",
    "    H.add_node(fam1)\n",
    "    for fam2 in range(fam1 + 1, number_of_families):\n",
    "        if fam2 in checked:\n",
    "            continue\n",
    "        G1 = nx.subgraph(G, conn_comps[fam1])\n",
    "        G2 = nx.subgraph(G, conn_comps[fam2])\n",
    "        if nx.is_isomorphic(G1, G2, node_match=lambda v1,v2: v1['label'] == v2['label'],\\\n",
    "                            edge_match=lambda e1,e2: e1['label'] == e2['label']):\n",
    "            checked.append(fam2)\n",
    "            H.add_edge(fam1, fam2)\n",
    "        print(str(fam1) + ' ' + str(fam2) + '   ', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "isomorphy_graph = open('glawinette_isomorphy_graph.p', 'wb')\n",
    "pickle.dump(H, isomorphy_graph)\n",
    "isomorphy_graph.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1201ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = pickle.load(open('glawinette_isomorphy_graph.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21134f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_conn_comps = [c for c in sorted(nx.connected_components(H), key=len, reverse=False)]\n",
    "print(len(H_conn_comps), 'groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c69a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_representative(K):\n",
    "    # the representative of a family: the node with the most degrees\n",
    "    max_degree = 0\n",
    "    selected_node = ''\n",
    "    for n in list(K.nodes):\n",
    "        if K.degree[n] > max_degree:\n",
    "            max_degree = K.degree[n]\n",
    "            selected_node = n\n",
    "        elif K.degree[n] == max_degree:\n",
    "            if len(selected_node) > len(n):\n",
    "                selected_node = n\n",
    "    return selected_node.split('_')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd351a",
   "metadata": {},
   "source": [
    "# Creation of txt file for each family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'glawinette_families'\n",
    "lexeme_dict = dict()\n",
    "f_summary = codecs.open('summary_of_glawinette_families.txt', 'w+', encoding='utf-8')\n",
    "f_summary.write('family_id\\tnumber_of_lexemes\\tlexemes\\n')\n",
    "for fam_fam_id, fam_fam in enumerate(H_conn_comps):\n",
    "    for fam_id, fam in enumerate(fam_fam):\n",
    "        representative = find_representative(nx.subgraph(G, conn_comps[fam]))\n",
    "        if len(fam_fam) == 1:\n",
    "            family_title = 'F' + str(fam_fam_id).rjust(5, '0')\n",
    "        else:\n",
    "            family_title = 'F' + str(fam_fam_id).rjust(5, '0') + '-' + str(fam_id).rjust(len(str(len(fam_fam)-1)), '0')\n",
    "        f_summary.write(family_title + '\\t'\\\n",
    "                       + str(len(conn_comps[fam])) + '\\t' + str(conn_comps[fam]) + '\\n')\n",
    "        for lexeme in conn_comps[fam]:\n",
    "            filename = family_title + ' ' + representative + '.txt'\n",
    "            lexeme_dict[lexeme] = filename\n",
    "            f_out = codecs.open(join(output_folder, filename), 'w+', encoding='utf-8')\n",
    "            f_out.write(header + '\\tfichier_origine' + '\\n\\n')\n",
    "            f_out.close()\n",
    "f_summary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('glawinette-series.csv', 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f):\n",
    "        if line_num < 1:\n",
    "            continue\n",
    "        elements = line.replace(' ','').split('\\t')\n",
    "        lexeme1 = elements[lemma1] + '_' + elements[cat1]\n",
    "        output_filename = lexeme_dict[lexeme1]\n",
    "        f_out = codecs.open(join(output_folder, output_filename), 'a+', encoding='utf-8')\n",
    "        f_out.write(line)\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f4ac1",
   "metadata": {},
   "source": [
    "# Creation of graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_writer(L):\n",
    "    ret_str = ''\n",
    "    edges = L.edges(data=True)\n",
    "    for e in edges:\n",
    "        category0 = e[0].split('_')[1]\n",
    "        category1 = e[1].split('_')[1]\n",
    "        ret_str += category0 + ' - ' + e[2]['label'] + ' - ' + category1 + '; '\n",
    "    return ret_str\n",
    "\n",
    "f_summary = codecs.open('summary_of_glawinette_groups.txt', 'w+', encoding='utf-8')\n",
    "f_summary.write('group id\\tnumber of lexemes\\tnumber of pairs\\tnumber of families\\tpairs\\tfamilies\\n')\n",
    "number_of_edges = []\n",
    "number_of_families = []\n",
    "words = ''\n",
    "group_prec = ''\n",
    "family_count = 0\n",
    "L = nx.Graph()\n",
    "input_dir = 'glawinette_families'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "for input_file in input_files:\n",
    "    group_id = input_file.split(' ')[0].split('-')[0]\n",
    "    if group_id != group_prec and group_prec != '':\n",
    "        f_summary.write(group_prec + '\\t' + str(len(L)) + '\\t' + str(L.size()) + '\\t' + str(family_count) + '\\t')\n",
    "        f_summary.write(edge_writer(L)[:-2])\n",
    "        f_summary.write('\\t' + words[:-2] + '\\n')\n",
    "        family_count = 0\n",
    "        words = ''\n",
    "    family_count += 1\n",
    "    group_prec = group_id\n",
    "    L = nx.Graph()\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                v1 = elements[lemma1] + '_' + elements[cat1]\n",
    "                v2 = elements[lemma2] + '_' + elements[cat2]\n",
    "                if L.has_edge(v1, v2):\n",
    "                    continue\n",
    "                L.add_node(v1, label=elements[lemma1] + '\\n' + elements[cat1])\n",
    "                L.add_node(v2, label=elements[lemma2] + '\\n' + elements[cat2])\n",
    "                pattern1 = FAPconverter(elements[FAP1])\n",
    "                pattern2 = FAPconverter(elements[FAP2])\n",
    "                sorted_pattern = sorted([pattern1, pattern2])\n",
    "                L.add_edge(v1, v2, label=sorted_pattern[0] + '-' + sorted_pattern[1])\n",
    "    words += str(list(L.nodes())) + '; '\n",
    "    write_dot(L, join('glawinette_visualization', input_file.replace('.txt','.dot')))\n",
    "    print(input_file.split(' ')[0], end='\\r')\n",
    "f_summary.write(group_prec + '\\t' + str(len(L)) + '\\t' + str(L.size()) + '\\t' + str(family_count) + '\\t')\n",
    "f_summary.write(edge_writer(L)[:-2])\n",
    "f_summary.write('\\t' + words[:-2] + '\\n')  \n",
    "f_summary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf426520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
