{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3da0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from networkx.algorithms import isomorphism\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "from utils import printProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cow_dict = dict()\n",
    "# counter = 0\n",
    "with codecs.open('frequencies-frcowvec-filtered.csv', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        elements = line.strip('\\n').split(',')\n",
    "        if elements[1] == 'freq':\n",
    "            continue\n",
    "        lexeme = elements[0].split('_')[0]\n",
    "        try:\n",
    "            if cow_dict[lexeme] < int(elements[1]):\n",
    "                cow_dict[lexeme] = int(elements[1])\n",
    "        except KeyError:\n",
    "            cow_dict[lexeme] = int(elements[1])\n",
    "#         print(counter, end='\\r')\n",
    "#         counter += 1\n",
    "# print(counter)\n",
    "print(len(cow_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedc3a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11273094\n"
     ]
    }
   ],
   "source": [
    "cow_set = set()\n",
    "with codecs.open('frequencies-frcowvec-filtered.csv', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        elements = line.strip('\\n').split(',')\n",
    "        if elements[1] == 'freq':\n",
    "            continue\n",
    "        cow_set.add(elements[0].split('_')[0])\n",
    "print(len(cow_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37c48639",
   "metadata": {},
   "outputs": [],
   "source": [
    "demonette_lexemes = set()\n",
    "with codecs.open('lexemes.csv', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        elements = line.split('\\t')\n",
    "        if elements[0] == 'lid':\n",
    "            continue\n",
    "        demonette_lexemes.add(elements[2])\n",
    "        \n",
    "glawi_constructions = list()\n",
    "with codecs.open('glawi-constructions.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        glawi_constructions.append(line.strip('\\n'))\n",
    "        \n",
    "lexemes_in_bow = set()\n",
    "with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lexeme = line.split()[0].split('_')[0]\n",
    "        lexemes_in_bow.add(lexeme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979341a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_average(vector_dict, lex2):\n",
    "    if len(vector_dict) == 0:\n",
    "        return 0\n",
    "    total_cosine = 0\n",
    "    with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            lexeme = line.split()[0].split('_')[0]\n",
    "            if lexeme == lex2:\n",
    "                v2 = np.array(list(map(lambda x: float(x), line.split()[1:])))\n",
    "                for v in vector_dict.values():\n",
    "                    total_cosine += dot(v, v2)/(norm(v)*norm(v2))\n",
    "                break\n",
    "    return round(total_cosine / len(vector_dict), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25663efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(pattern, word):  # pattern = 'preXisation', word = 'precognisation' => True\n",
    "    if pattern == 'X':\n",
    "        return True\n",
    "    counter = 0\n",
    "    try:\n",
    "        for c in pattern:\n",
    "            if c == 'X':\n",
    "                break\n",
    "            if c != word[counter]:\n",
    "                return False\n",
    "            counter += 1\n",
    "        counter = -1\n",
    "        while True:\n",
    "            if pattern[counter] == 'X':\n",
    "                break\n",
    "            if pattern[counter] != word[counter]:\n",
    "                return False\n",
    "            counter -= 1\n",
    "    except IndexError:  # Xtractif & actif\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def generate_lexemes(nodes):\n",
    "    return_dict = dict()\n",
    "    return_list = list()  # list of tuples [(word, cosine)]\n",
    "    lexemes = set()\n",
    "    vector_dict = dict()\n",
    "    count_not_in_bow = 0\n",
    "    for n in nodes:\n",
    "        lexeme = n.split('_')[0]\n",
    "        if lexeme not in lexemes_in_bow and lexeme not in lexemes:\n",
    "            count_not_in_bow += 1\n",
    "        lexemes.add(lexeme)\n",
    "#         vector_dict[lexeme] = np.array([])\n",
    "    count_lexemes = len(lexemes) - count_not_in_bow\n",
    "    with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if count_lexemes <= 0:\n",
    "                break\n",
    "            lexeme = line.split()[0].split('_')[0]\n",
    "            if lexeme in lexemes:\n",
    "                vector_dict[lexeme] = np.array(list(map(lambda x: float(x), line.split()[1:])))\n",
    "                count_lexemes -= 1\n",
    "    for lexeme in lexemes:\n",
    "        best_const_length = -1\n",
    "        best_const = ''\n",
    "        for c in glawi_constructions:\n",
    "            const1 = c.split('-')[0]\n",
    "            if len(const1) > best_const_length and match(const1, lexeme):\n",
    "                best_const_length = len(const1)\n",
    "                best_const = const1\n",
    "        for c in glawi_constructions:\n",
    "            [const1, const2] = c.split('-')\n",
    "            if const1 != best_const:\n",
    "                continue\n",
    "            [prefix, postfix] = const1.split('X')\n",
    "            stem = lexeme.replace(prefix, '', 1)\n",
    "            if postfix:  # if not empty\n",
    "                stem = ''.join(stem.rsplit(postfix, 1))\n",
    "            new_lexeme = const2.replace('X', stem)\n",
    "            if new_lexeme in return_dict or new_lexeme in lexemes:\n",
    "                continue\n",
    "            if new_lexeme in demonette_lexemes or new_lexeme in cow_set:\n",
    "                if new_lexeme in lexemes_in_bow:\n",
    "                    return_dict[new_lexeme] = cosine_average(vector_dict, new_lexeme)\n",
    "                else:\n",
    "                    return_dict[new_lexeme] = -2\n",
    "                return_list.append((new_lexeme, return_dict[new_lexeme]))\n",
    "#             try:\n",
    "#                 return_dict[new_lexeme] = cow_dict[new_lexeme]\n",
    "#             except KeyError:\n",
    "#                 if new_lexeme in demonette_lexemes:\n",
    "#                     return_dict[new_lexeme] = 0\n",
    "    return_list.sort(key=lambda x:x[1], reverse=True)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b17593fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dir = 'demonette-glawinette_graph_binary'\n",
    "input_files = [f for f in listdir(binary_dir) if isfile(join(binary_dir, f))]\n",
    "input_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F00103\r"
     ]
    }
   ],
   "source": [
    "output_file = codecs.open('DG_propositions_and_cosineX.txt', 'a+', encoding='utf-8')\n",
    "counter_file = -1\n",
    "for input_file in input_files:\n",
    "    print(input_file, end='\\r')\n",
    "    counter_file += 1\n",
    "    if counter_file < 82:  #50\n",
    "        continue\n",
    "    G = pickle.load(open(join(binary_dir, input_file), 'rb'))\n",
    "    output_file.write(input_file + '\\t' + str(generate_lexemes(G.nodes())) + '\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = pickle.load(open(join(binary_dir, 'F00082'), 'rb'))\n",
    "generate_lexemes(G.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b611ef",
   "metadata": {},
   "source": [
    "# reading vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexemes_in_bow = set()\n",
    "with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lexeme = line.split()[0].split('_')[0]\n",
    "        lexemes_in_bow.add(lexeme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(lex1, lex2):\n",
    "    if lex1 not in lexemes_in_bow or lex2 not in lexemes_in_bow:\n",
    "        return -2\n",
    "    found1 = False\n",
    "    found2 = False\n",
    "    with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            lexeme = line.split()[0].split('_')[0]\n",
    "            if lexeme == lex1:\n",
    "                print(line)\n",
    "                v1 = np.array(list(map(lambda x: float(x), line.split()[1:])))\n",
    "                found1 = True\n",
    "                if found2:\n",
    "                    break\n",
    "            elif lexeme == lex2:\n",
    "                print(line)\n",
    "                v2 = np.array(list(map(lambda x: float(x), line.split()[1:])))\n",
    "                found2 = True\n",
    "                if found1:\n",
    "                    break\n",
    "    if found1 and found2:\n",
    "        return dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    else:\n",
    "        return -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5069b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine('aimable', 'aimant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e0e81",
   "metadata": {},
   "source": [
    "# reading vector space to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27781de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = -1\n",
    "bow_dict = dict()\n",
    "with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        counter += 1\n",
    "        if counter == 0:\n",
    "            print(line)\n",
    "            continue\n",
    "        if counter % 100 == 0:\n",
    "            print(counter, end='\\r')\n",
    "        lexeme = line.split()[0].split('_')[0]\n",
    "        v = np.array(list(map(lambda x: float(x), line.split()[1:])))\n",
    "        bow_dict[lexeme] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00246e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e52ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
