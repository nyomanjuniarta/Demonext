{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3da0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from networkx.algorithms import isomorphism\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "from utils import printProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cow_dict = dict()\n",
    "# with codecs.open('frequencies-frcowvec-filtered.csv', 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         elements = line.strip('\\n').split(',')\n",
    "#         if elements[1] == 'freq':\n",
    "#             continue\n",
    "#         lexeme = elements[0].split('_')[0]\n",
    "#         try:\n",
    "#             if cow_dict[lexeme] < int(elements[1]):\n",
    "#                 cow_dict[lexeme] = int(elements[1])\n",
    "#         except KeyError:\n",
    "#             cow_dict[lexeme] = int(elements[1])\n",
    "# print(len(cow_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c48639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexemes_in_cow = set()\n",
    "# with codecs.open('frequencies-frcowvec-filtered.csv', 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         elements = line.strip('\\n').split(',')\n",
    "#         if elements[1] == 'freq':\n",
    "#             continue\n",
    "#         lexemes_in_cow.add(elements[0].split('_')[0])\n",
    "# print(len(lexemes_in_cow))\n",
    "\n",
    "# lexemes_in_demTable = set()\n",
    "# with codecs.open('lexemes.csv', 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         elements = line.split('\\t')\n",
    "#         if elements[0] == 'lid':\n",
    "#             continue\n",
    "#         lexemes_in_demTable.add(elements[2])\n",
    "        \n",
    "glawi_constructions = list()\n",
    "with codecs.open('glawi-constructions.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        glawi_constructions.append(line.strip('\\n'))\n",
    "        \n",
    "lexemes_in_bow = set()\n",
    "with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lexeme = line.split()[0].split('_')[0]\n",
    "        lexemes_in_bow.add(lexeme)\n",
    "\n",
    "proposed_lexemes = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979341a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cosine_average(vector_dict, lex2):\n",
    "#     if len(vector_dict) == 0:\n",
    "#         return 0\n",
    "#     total_cosine = 0\n",
    "#     with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             lexeme = line.split()[0].split('_')[0]\n",
    "#             if lexeme == lex2:\n",
    "#                 v2 = np.array(list(map(lambda x: float(x), line.split()[1:])))\n",
    "#                 for v in vector_dict.values():\n",
    "#                     total_cosine += dot(v, v2)/(norm(v)*norm(v2))\n",
    "#                 break\n",
    "#     return round(total_cosine / len(vector_dict), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25663efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(pattern, word):  # pattern = 'preXisation', word = 'precognisation' => True\n",
    "    if pattern == 'X':\n",
    "        return True\n",
    "    counter = 0\n",
    "    try:\n",
    "        for c in pattern:\n",
    "            if c == 'X':\n",
    "                break\n",
    "            if c != word[counter]:\n",
    "                return False\n",
    "            counter += 1\n",
    "        counter = -1\n",
    "        while True:\n",
    "            if pattern[counter] == 'X':\n",
    "                break\n",
    "            if pattern[counter] != word[counter]:\n",
    "                return False\n",
    "            counter -= 1\n",
    "    except IndexError:  # Xtractif & actif\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def generate_lexemes(nodes):\n",
    "    return_set = set()\n",
    "    existing_lexemes = set()\n",
    "    for n in nodes:\n",
    "        existing_lexemes.add(n.split('_')[0])\n",
    "    for lexeme in existing_lexemes:\n",
    "        best_const_length = -1\n",
    "        best_const = ''\n",
    "        for c in glawi_constructions:\n",
    "            const1 = c.split('-')[0]\n",
    "            if len(const1) > best_const_length and match(const1, lexeme):\n",
    "                best_const_length = len(const1)\n",
    "                best_const = const1\n",
    "        for c in glawi_constructions:\n",
    "            [const1, const2] = c.split('-')\n",
    "            if const1 != best_const:\n",
    "                continue\n",
    "            [prefix, postfix] = const1.split('X')\n",
    "            stem = lexeme.replace(prefix, '', 1)\n",
    "            if postfix:  # if not empty\n",
    "                stem = ''.join(stem.rsplit(postfix, 1))\n",
    "            new_lexeme = const2.replace('X', stem)\n",
    "            if new_lexeme in return_set or new_lexeme in existing_lexemes:\n",
    "                continue\n",
    "            if new_lexeme in lexemes_in_bow:\n",
    "                return_set.add(new_lexeme)\n",
    "    return return_set\n",
    "\n",
    "# def generate_lexemes_with_cosine(nodes):\n",
    "#     return_dict = dict()\n",
    "#     return_list = list()  # list of tuples [(word, cosine)]\n",
    "#     lexemes = set()\n",
    "#     vector_dict = dict()\n",
    "#     count_not_in_bow = 0\n",
    "#     for n in nodes:\n",
    "#         lexeme = n.split('_')[0]\n",
    "#         if lexeme not in lexemes_in_bow and lexeme not in lexemes:\n",
    "#             count_not_in_bow += 1\n",
    "#         lexemes.add(lexeme)\n",
    "#     count_lexemes = len(lexemes) - count_not_in_bow\n",
    "#     with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             if count_lexemes <= 0:\n",
    "#                 break\n",
    "#             lexeme = line.split()[0].split('_')[0]\n",
    "#             if lexeme in lexemes:\n",
    "#                 vector_dict[lexeme] = np.array(list(map(lambda x: float(x), line.split()[1:])))\n",
    "#                 count_lexemes -= 1\n",
    "#     for lexeme in lexemes:\n",
    "#         best_const_length = -1\n",
    "#         best_const = ''\n",
    "#         for c in glawi_constructions:\n",
    "#             const1 = c.split('-')[0]\n",
    "#             if len(const1) > best_const_length and match(const1, lexeme):\n",
    "#                 best_const_length = len(const1)\n",
    "#                 best_const = const1\n",
    "#         for c in glawi_constructions:\n",
    "#             [const1, const2] = c.split('-')\n",
    "#             if const1 != best_const:\n",
    "#                 continue\n",
    "#             [prefix, postfix] = const1.split('X')\n",
    "#             stem = lexeme.replace(prefix, '', 1)\n",
    "#             if postfix:  # if not empty\n",
    "#                 stem = ''.join(stem.rsplit(postfix, 1))\n",
    "#             new_lexeme = const2.replace('X', stem)\n",
    "#             if new_lexeme in return_dict or new_lexeme in lexemes:\n",
    "#                 continue\n",
    "#             if new_lexeme in demonette_lexemes or new_lexeme in cow_set:\n",
    "#                 if new_lexeme in lexemes_in_bow:\n",
    "#                     return_dict[new_lexeme] = cosine_average(vector_dict, new_lexeme)\n",
    "#                 else:\n",
    "#                     return_dict[new_lexeme] = -2\n",
    "#                 return_list.append((new_lexeme, return_dict[new_lexeme]))\n",
    "#     return_list.sort(key=lambda x:x[1], reverse=True)\n",
    "#     return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17593fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dir = 'DG-graph-binary'\n",
    "input_files = [f for f in listdir(binary_dir) if isfile(join(binary_dir, f))]\n",
    "input_files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0dba79",
   "metadata": {},
   "source": [
    "## generate propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a565c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F08068-5283\r"
     ]
    }
   ],
   "source": [
    "output_file = codecs.open('DG_propositions.txt', 'a+', encoding='utf-8')\n",
    "counter_file = -1\n",
    "for input_file in input_files:\n",
    "    print(input_file, end='\\r')\n",
    "    counter_file += 1\n",
    "#     if counter_file < 82:\n",
    "#         continue\n",
    "    G = pickle.load(open(join(binary_dir, input_file), 'rb'))\n",
    "    propositions = generate_lexemes(G.nodes())\n",
    "    proposed_lexemes.update(propositions)\n",
    "    output_file.write(input_file + '\\t' + str(propositions) + '\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(proposed_lexemes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f04d6",
   "metadata": {},
   "source": [
    "## max and average of cosine similarity in each family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59046867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F08068-5283\r"
     ]
    }
   ],
   "source": [
    "lexemes_in_demonette_families = set()\n",
    "for file in input_files:\n",
    "    G = pickle.load(open(join(binary_dir, file), 'rb'))\n",
    "    for n in G.nodes():\n",
    "        lexemes_in_demonette_families.add(n.split('_')[0])\n",
    "    print(file, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662dac26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2149805 vectors in bow\n",
      "93550 vectors kept\n"
     ]
    }
   ],
   "source": [
    "vector_dict = dict()\n",
    "counter = 0\n",
    "with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lexeme = line.split()[0].split('_')[0]\n",
    "        if lexeme in lexemes_in_demonette_families or lexeme in proposed_lexemes:\n",
    "            v = np.array(list(map(lambda x: float(x), line.split()[1:])))\n",
    "            vector_dict[lexeme] = v\n",
    "        print(counter, end='\\r')\n",
    "        counter += 1\n",
    "print(counter, 'vectors in bow')\n",
    "print(len(vector_dict), 'vectors kept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7fae8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F08068-5283\r"
     ]
    }
   ],
   "source": [
    "output_file = codecs.open('DG_family_cosine.txt', 'w', encoding='utf-8')\n",
    "output_file.write('familyID\\tmax_cosine\\tavg_cosine\\tlexemes\\n')\n",
    "for file in input_files:\n",
    "    G = pickle.load(open(join(binary_dir, file), 'rb'))\n",
    "    max_cos = -3\n",
    "    comparison_counter = 0\n",
    "    total_cosine = 0\n",
    "    nodes = list(G.nodes())\n",
    "    for n1 in range(0, len(nodes) - 1):\n",
    "        for n2 in range(n1 + 1, len(nodes) - 1):\n",
    "            try:\n",
    "                lex1 = nodes[n1].split('_')[0]\n",
    "                lex2 = nodes[n2].split('_')[0]\n",
    "                if lex1 == lex2:\n",
    "                    continue\n",
    "                vec1 = vector_dict[lex1]\n",
    "                vec2 = vector_dict[lex2]\n",
    "                cosine_similarity = dot(vec1, vec2)/(norm(vec1)*norm(vec2))\n",
    "                if cosine_similarity > max_cos:\n",
    "                    max_cos = cosine_similarity\n",
    "                total_cosine += cosine_similarity\n",
    "                comparison_counter += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "    if comparison_counter == 0:\n",
    "        output_file.write(file + '\\t?\\t0\\t' + str(nodes) + '\\n')\n",
    "    else:\n",
    "        output_file.write(file + '\\t' + str(round(max_cos, 2)) + '\\t' + str(round(total_cosine/comparison_counter, 2))\\\n",
    "                      + '\\t' + str(nodes) + '\\n')\n",
    "    print(file, end='\\r')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b611ef",
   "metadata": {},
   "source": [
    "# reading vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dir = 'DG-graph-binary'\n",
    "input_files = [f for f in listdir(binary_dir) if isfile(join(binary_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f9ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexemes_in_demonette = set()\n",
    "for file in input_files:\n",
    "    G = pickle.load(open(join(binary_dir, file), 'rb'))\n",
    "    for n in G.nodes():\n",
    "        lexemes_in_demonette.add(n.split('_')[0])\n",
    "    print(file, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d3713",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lexemes_in_demonette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexemes_in_bow = set()\n",
    "counter = 0\n",
    "categories = set()\n",
    "with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lexeme = line.split()[0].split('_')[0]\n",
    "        if lexeme in lexemes_in_demonette:\n",
    "            lexemes_in_bow.add(lexeme)\n",
    "        print(counter, end='\\r')\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e02fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lexemes_in_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e52ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
