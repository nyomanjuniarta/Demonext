{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from utils import printProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'pairs'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "\n",
    "input_file_code = dict()\n",
    "input_file_code['converts'] = 'C'\n",
    "input_file_code['demonette1'] = 'D'\n",
    "\n",
    "input_file_code['denomCONVX'] = 'Na'\n",
    "input_file_code['denomPREFX'] = 'Nb'\n",
    "input_file_code['denomXaire'] = 'Nc'\n",
    "input_file_code['denomXal'] = 'Nd'\n",
    "input_file_code['denomXique'] = 'Ne'\n",
    "input_file_code['denomXSUF1'] = 'Nf'\n",
    "input_file_code['denomXSUF2'] = 'Ng'\n",
    "input_file_code['denomXSUF3'] = 'Nh'\n",
    "input_file_code['denomXSUF4'] = 'Ni'\n",
    "input_file_code['denomXSUF5'] = 'Nj'\n",
    "input_file_code['denomXSUF6'] = 'Nk'\n",
    "\n",
    "input_file_code['dimocXaie'] = 'Ma'\n",
    "input_file_code['dimocXat'] = 'Mb'\n",
    "input_file_code['dimocXet'] = 'Mc'\n",
    "input_file_code['dimocXier'] = 'Md'\n",
    "\n",
    "input_file_code['derifantiX'] = 'Ra'\n",
    "input_file_code['derifde1X'] = 'Rb'\n",
    "input_file_code['derifenX'] = 'Rc'\n",
    "input_file_code['derifinX'] = 'Rd'\n",
    "input_file_code['derifQUANTX'] = 'Re'\n",
    "input_file_code['derifreX'] = 'Rf'\n",
    "input_file_code['deriftriX'] = 'Rg'\n",
    "input_file_code['derifXable'] = 'Rh'\n",
    "input_file_code['derifXiser'] = 'Ri'\n",
    "\n",
    "# column number\n",
    "graph_1 = 3\n",
    "graph_2 = 6\n",
    "cat_1 = 8\n",
    "cat_2 = 10\n",
    "cstr_1 = 14\n",
    "cstr_2 = 17\n",
    "complexite = 19\n",
    "orientation = 21\n",
    "fichier_origine = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_representative(K):\n",
    "    # a representative of a family: the root (no direct relation going in), or the node with the largest outdegree\n",
    "    in_degree_dict = dict()  # only counting 'des2as' and 'as2des'\n",
    "    for v in list(K.nodes):\n",
    "        in_degree_dict[v] = 0\n",
    "    for e in list(K.edges.data()):\n",
    "        if not ('NA' in e[2]['label'] or 'indirect' in e[2]['label']):\n",
    "            in_degree_dict[e[1]] += 1\n",
    "    roots = list()\n",
    "    for k in in_degree_dict:\n",
    "        if in_degree_dict[k] == 0:\n",
    "            roots.append(k)\n",
    "    if len(roots) == 1:\n",
    "        return roots[0].split('_')[0]\n",
    "    elif len(roots) == 0:\n",
    "        max_out_degree = 0\n",
    "        selected_node = ''\n",
    "        for n in list(K.nodes):\n",
    "            if K.out_degree(n) > max_out_degree:\n",
    "                max_out_degree = K.out_degree(n)\n",
    "                selected_node = n\n",
    "        return selected_node.split('_')[0]\n",
    "    elif len(roots) > 1:\n",
    "        max_out_degree = 0\n",
    "        selected_root = roots[0]\n",
    "        for r in roots:\n",
    "            if K.out_degree(r) > max_out_degree:\n",
    "                max_out_degree = K.out_degree(r)\n",
    "                selected_root = r\n",
    "        return selected_root.split('_')[0]\n",
    "    \n",
    "def category_shortening(cat):\n",
    "    if cat != 'Num' and cat[0] == 'N':\n",
    "        return 'N'\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103377 pairs\n",
      "51830 unique pairs\n"
     ]
    }
   ],
   "source": [
    "header = ''\n",
    "G = nx.DiGraph()\n",
    "number_of_pairs = 0\n",
    "number_of_unique_pairs = 0\n",
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num == 0:\n",
    "                header = line.replace('\\n','')\n",
    "            elif line_num >= 2:\n",
    "                number_of_pairs += 1\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                v1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "                v2 = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if G.has_edge(v1, v2) or G.has_edge(v2, v1):\n",
    "                    continue\n",
    "                G.add_node(elements[graph_1] + '_' + elements[cat_1], label=category_shortening(elements[cat_1]))\n",
    "                G.add_node(elements[graph_2] + '_' + elements[cat_2], label=category_shortening(elements[cat_2]))\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    G.add_edge(v1, v2, label=elements[cstr_1] + '-' + elements[cstr_2]) # without complexity\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    G.add_edge(v2, v1, label=elements[cstr_2] + '-' + elements[cstr_1])\n",
    "                else:\n",
    "                    #sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    G.add_edge(v1, v2, label=elements[cstr_1] + '-' + elements[cstr_2] + '_' + elements[orientation])\n",
    "                    G.add_edge(v2, v1, label=elements[cstr_2] + '-' + elements[cstr_1] + '_' + elements[orientation])\n",
    "                number_of_unique_pairs += 1\n",
    "print(number_of_pairs, 'pairs')\n",
    "print(number_of_unique_pairs, 'unique pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13897"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_comps = list(nx.weakly_connected_components(G))\n",
    "number_of_families = len(conn_comps)\n",
    "number_of_families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |█████████████████████████████████████████████████-| 99.99% complete\r"
     ]
    }
   ],
   "source": [
    "checked = list()\n",
    "H = nx.Graph() # graph of graphs\n",
    "for fam1 in range(0, number_of_families):\n",
    "    if fam1 in checked:\n",
    "        continue\n",
    "    H.add_node(fam1)\n",
    "    for fam2 in range(fam1 + 1, number_of_families):\n",
    "        if fam2 in checked:\n",
    "            continue\n",
    "        G1 = nx.subgraph(G, conn_comps[fam1])\n",
    "        G2 = nx.subgraph(G, conn_comps[fam2])\n",
    "        if nx.is_isomorphic(G1, G2, node_match=lambda v1,v2: v1['label'] == v2['label'],\\\n",
    "                            edge_match=lambda e1,e2: e1['label'] == e2['label']):\n",
    "            checked.append(fam2)\n",
    "            H.add_edge(fam1, fam2)\n",
    "    printProgressBar(fam1 + 1, number_of_families, prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomorphy_graph = open('isomorphy_graph.p', 'wb')\n",
    "pickle.dump(H, isomorphy_graph)\n",
    "isomorphy_graph.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = pickle.load(open('isomorphy_graph.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4849\n"
     ]
    }
   ],
   "source": [
    "#H_conn_comps = list(nx.connected_components(H))\n",
    "H_conn_comps = [c for c in sorted(nx.connected_components(H), key=len, reverse=False)]\n",
    "print(len(H_conn_comps))\n",
    "#H_conn_comps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'families'\n",
    "lexeme_dict = dict()\n",
    "f_summary = codecs.open('summary_of_families.txt', 'w+', encoding='utf-8')\n",
    "f_summary.write('family_id\\tnumber_of_lexemes\\tlexemes\\n')\n",
    "for fam_fam_id, fam_fam in enumerate(H_conn_comps):\n",
    "    for fam_id, fam in enumerate(fam_fam):\n",
    "        # first_member = sorted(list(conn_comps[fam]))[0]\n",
    "        representative = find_representative(nx.subgraph(G, conn_comps[fam]))\n",
    "        if len(fam_fam) == 1:\n",
    "            family_title = 'F' + str(fam_fam_id).rjust(5, '0')\n",
    "        else:\n",
    "            family_title = 'F' + str(fam_fam_id).rjust(5, '0') + '-' + str(fam_id).rjust(len(str(len(fam_fam)-1)), '0')\n",
    "        f_summary.write(family_title + '\\t'\\\n",
    "                       + str(len(conn_comps[fam])) + '\\t' + str(conn_comps[fam]) + '\\n')\n",
    "        for lexeme in conn_comps[fam]:\n",
    "            filename = family_title + ' ' + representative + '.txt'\n",
    "            lexeme_dict[lexeme] = filename\n",
    "            f_out = codecs.open(join(output_folder, filename), 'w+', encoding='utf-8')\n",
    "            f_out.write(header + '\\tfichier_origine' + '\\n\\n')\n",
    "            f_out.close()\n",
    "f_summary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num < 2:\n",
    "                continue\n",
    "            elements = line.replace(' ','').split('\\t')\n",
    "            lexeme1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "            if elements[complexite] not in ['simple', 'complexe', 'motiv-form', 'motiv-sem', 'accidentel']:\n",
    "                print('warning ', input_file)\n",
    "            output_filename = lexeme_dict[lexeme1]\n",
    "            f_out = codecs.open(join(output_folder, output_filename), 'a+', encoding='utf-8')\n",
    "            f_out.write(line.strip('\\n') + '\\t' + input_file_code.get(input_file.split('-')[0]) + '\\n')\n",
    "            f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get all categories, lexemes, orientation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centans\n",
      "milleans\n",
      "fauxjumeau\n",
      "caractèrefemelle\n",
      "caractèrefemelle\n",
      "centans\n",
      "caractèremâle\n",
      "caractèremâle\n",
      "caractèremâle\n",
      "êtrevivant\n",
      "fauxjumeau\n",
      "categories: {'Npm', 'Nm', 'Nf', 'Adj', 'Nfp', 'Nx', 'Pro', 'Npf', 'V', 'Nmp', 'Npx', 'Num', 'More', 'Adv'}\n"
     ]
    }
   ],
   "source": [
    "header = ''\n",
    "list_of_categories = set()\n",
    "lexemes_with_nonalpha = set()\n",
    "input_dir = 'pairs'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num == 0:\n",
    "                header = line.replace('\\n','')\n",
    "            elif line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                if elements[cat_2] == 'More':\n",
    "                    print(elements[graph_2])\n",
    "                list_of_categories.add(elements[cat_1])\n",
    "                list_of_categories.add(elements[cat_2])\n",
    "                if not elements[graph_1].isalpha():\n",
    "                    lexemes_with_nonalpha.add(elements[graph_1])\n",
    "                if not elements[graph_2].isalpha():\n",
    "                    lexemes_with_nonalpha.add(elements[graph_2])\n",
    "print('categories: ' + str(list_of_categories))\n",
    "#print(lexemes_with_nonalpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of double arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_direction(input_orientation):\n",
    "    if input_orientation == 'de2as':\n",
    "        input_orientation = 'as2de'\n",
    "    elif input_orientation == 'as2de':\n",
    "        input_orientation = 'de2as'\n",
    "    return input_orientation\n",
    "\n",
    "\n",
    "def line_writer(input_line):\n",
    "    elements = line.replace('\\n','').replace(' ','').replace('des2as', 'de2as').replace('as2des', 'as2de').split('\\t')\n",
    "    ret_line = elements[graph_1] + '\\t' + elements[cat_1] + '\\t' + elements[graph_2] + '\\t' + elements[cat_2] + '\\t'\\\n",
    "    + elements[orientation] + '\\t' + elements[complexite] + '\\t' + elements[cstr_1] + '\\t' + elements[cstr_2] + '\\t'\\\n",
    "    + elements[fichier_origine] + '\\n'\n",
    "    return ret_line\n",
    "\n",
    "\n",
    "f_duplicates = codecs.open('paires_doublons.txt', 'w+', encoding='utf-8')\n",
    "f_duplicates.write('graph_1\\tcat_1\\tgraph_2\\tcat_2\\torientation\\tcomplexite\\tcstr_1\\tcstr_2\\tfichier_origine\\n')\n",
    "input_dir = 'families'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "counter = 0\n",
    "for input_file in input_files:\n",
    "    G = nx.Graph()\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                reverse = False\n",
    "                elements = line.replace('\\n','').replace(' ','').replace('des2as', 'de2as').replace('as2des', 'as2de').split('\\t')\n",
    "                v1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "                v2 = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if v1 > v2:\n",
    "                    reverse = True\n",
    "                if reverse:\n",
    "                    edge_label = elements[complexite] + '_' + reverse_direction(elements[orientation]) + '_'\\\n",
    "                    + elements[cstr_2] + '-' + elements[cstr_1]\n",
    "                else:\n",
    "                    edge_label = elements[complexite] + '_' + elements[orientation] + '_' \\\n",
    "                    + elements[cstr_1] + '-' + elements[cstr_2]\n",
    "                if G.has_edge(v1, v2) and G[v1][v2]['label'] != edge_label and not G[v1][v2]['checked']:\n",
    "                    f_duplicates.write(G[v1][v2]['complete_line'])\n",
    "                    f_duplicates.write(line_writer(line) + '\\n')\n",
    "                    G[v1][v2]['checked'] = True\n",
    "                elif not G.has_edge(v1, v2):\n",
    "                    G.add_edge(v1, v2, label=edge_label, complete_line=line_writer(line), checked=False)\n",
    "    counter += 1\n",
    "    printProgressBar(counter + 1, len(input_files), prefix='Progress:', suffix='complete', length=50, decimals=2)\n",
    "f_duplicates.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fingerprints\n",
    "Category is used when creating nodes (to avoid joining two lexemes with same graphie but different category), but not for comparing isomorphy.\n",
    "\n",
    "Ignoring pairs with \"indirect\" and \"NA\" as orientation -> possible for some lexemes to be isolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ''\n",
    "G = nx.DiGraph()\n",
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num == 0:\n",
    "                header = line.replace('\\n','')\n",
    "            elif line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                G.add_node(elements[graph_1] + '_' + elements[cat_1], label='')\n",
    "                G.add_node(elements[graph_2] + '_' + elements[cat_2], label='')\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    edge_type = elements[cstr_1] + '-' + elements[cstr_2]\n",
    "                    G.add_edge(elements[graph_1] + '_' + elements[cat_1], elements[graph_2] + '_' + elements[cat_2],\\\n",
    "                               label=edge_type)\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    edge_type = elements[cstr_2] + '-' + elements[cstr_1]\n",
    "                    G.add_edge(elements[graph_2] + '_' + elements[cat_2], elements[graph_1] + '_' + elements[cat_1],\\\n",
    "                               label=edge_type)\n",
    "                elif elements[orientation] == 'NA':\n",
    "                    edge_type = elements[cstr_1] + '-' + elements[cstr_2]\n",
    "                    G.add_edge(elements[graph_1] + '_' + elements[cat_1], elements[graph_2] + '_' + elements[cat_2],\\\n",
    "                               label=edge_type)\n",
    "                    edge_type = elements[cstr_2] + '-' + elements[cstr_1]\n",
    "                    G.add_edge(elements[graph_2] + '_' + elements[cat_2], elements[graph_1] + '_' + elements[cat_1],\\\n",
    "                               label=edge_type)\n",
    "                else:  # orientation: indirect\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_comps = list(nx.weakly_connected_components(G))\n",
    "number_of_families = len(conn_comps)\n",
    "number_of_families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked = list()\n",
    "H = nx.Graph() # graph of graphs\n",
    "for fam1 in range(0, number_of_families):\n",
    "    if fam1 in checked:\n",
    "        continue\n",
    "    H.add_node(fam1)\n",
    "    G1 = nx.subgraph(G, conn_comps[fam1])\n",
    "    if len(G1) == 1:  # isolated\n",
    "        continue\n",
    "    for fam2 in range(fam1 + 1, number_of_families):\n",
    "        if fam2 in checked:\n",
    "            continue\n",
    "        G2 = nx.subgraph(G, conn_comps[fam2])\n",
    "        if len(G2) == 1:  # isolated\n",
    "            continue\n",
    "        if nx.is_isomorphic(G1, G2, node_match=lambda v1,v2: v1['label'] == v2['label'],\\\n",
    "                            edge_match=lambda e1,e2: e1['label'] == e2['label']):\n",
    "            checked.append(fam2)\n",
    "            H.add_edge(fam1, fam2)\n",
    "    printProgressBar(fam1 + 1, number_of_families, prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isomorphy_fingerprint = open('isomorphy_fingerprint.p', 'wb')\n",
    "pickle.dump(H, isomorphy_fingerprint)\n",
    "isomorphy_fingerprint.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = pickle.load(open('isomorphy_fingerprint.p', 'rb'))\n",
    "#H_conn_comps = list(nx.connected_components(H))\n",
    "H_conn_comps = [c for c in sorted(nx.connected_components(H), key=len, reverse=False)]\n",
    "print(len(H_conn_comps))\n",
    "#H_conn_comps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'families_fingerprint'\n",
    "lexeme_dict = dict()\n",
    "f_summary = codecs.open('summary_of_families_from_fingerprint.txt', 'w+', encoding='utf-8')\n",
    "f_summary.write('family_id\\tnumber_of_lexemes\\tlexemes\\n')\n",
    "for fam_fam_id, fam_fam in enumerate(H_conn_comps):\n",
    "    for fam_id, fam in enumerate(fam_fam):\n",
    "        # first_member = sorted(list(conn_comps[fam]))[0]\n",
    "        representative = find_representative(nx.subgraph(G, conn_comps[fam]))\n",
    "        if len(fam_fam) == 1:\n",
    "            family_title = 'F' + str(fam_fam_id).rjust(5, '0')\n",
    "        else:\n",
    "            family_title = 'F' + str(fam_fam_id).rjust(5, '0') + '-' + str(fam_id).rjust(len(str(len(fam_fam)-1)), '0')\n",
    "        f_summary.write(family_title + '\\t'\\\n",
    "                       + str(len(conn_comps[fam])) + '\\t' + str(conn_comps[fam]) + '\\n')\n",
    "        for lexeme in conn_comps[fam]:\n",
    "            filename = family_title + ' ' + representative + '.txt'\n",
    "            lexeme_dict[lexeme] = filename\n",
    "            f_out = codecs.open(join(output_folder, filename), 'w+', encoding='utf-8')\n",
    "            f_out.write(header + '\\tfichier_origine' + '\\n\\n')\n",
    "            f_out.close()\n",
    "f_summary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num < 2:\n",
    "                continue\n",
    "            elements = line.replace(' ','').split('\\t')\n",
    "            lexeme1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "            if elements[complexite] not in ['simple', 'complexe', 'motiv-form', 'motiv-sem', 'accidentel']:\n",
    "                print('warning ', input_file)\n",
    "            output_filename = lexeme_dict[lexeme1]\n",
    "            f_out = codecs.open(join(output_folder, output_filename), 'a+', encoding='utf-8')\n",
    "            f_out.write(line.strip('\\n') + '\\t' + input_files[input_file] + '\\n')\n",
    "            f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
