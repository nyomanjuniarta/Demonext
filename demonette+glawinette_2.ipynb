{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d5f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import networkx as nx\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from networkx.algorithms import isomorphism\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "from utils import printProgressBar\n",
    "\n",
    "# glawinette column number\n",
    "lemma1 = 0\n",
    "lemma2 = 1\n",
    "cat1 = 2\n",
    "cat2 = 3\n",
    "origine_morpho = 4\n",
    "origine_def = 5\n",
    "BAP1 = 6\n",
    "BAP2 = 7\n",
    "BAPsize = 8\n",
    "FAP1 = 9\n",
    "FAP2 = 10\n",
    "FAPsize = 11\n",
    "radical = 12\n",
    "FAPtype = 13\n",
    "\n",
    "# demonette column number\n",
    "graph_1 = 3\n",
    "graph_2 = 6\n",
    "cat_1 = 8\n",
    "cat_2 = 10\n",
    "cstr_1 = 14\n",
    "cstr_2 = 17\n",
    "complexite = 19\n",
    "orientation = 21\n",
    "fichier_origine = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FAPconverter(input_fap):\n",
    "    return input_fap.replace('(.+)', 'X').replace('$', '').replace('^', '')\n",
    "\n",
    "def category_shortening(cat):\n",
    "    if cat != 'Num' and cat[0] == 'N':\n",
    "        if cat[1] == 'p':  # nom propre\n",
    "            return 'Np'\n",
    "        return 'N'  # nom\n",
    "    return cat\n",
    "\n",
    "header = ''\n",
    "glawi_dict = dict()\n",
    "with codecs.open('glawinette-series.csv', 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f):\n",
    "        if line_num >= 1:\n",
    "            elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "            glawi_dict[(elements[lemma1], elements[lemma2])] = FAPconverter(elements[FAP1]) + '-' + FAPconverter(elements[FAP2])\n",
    "            glawi_dict[(elements[lemma2], elements[lemma1])] = FAPconverter(elements[FAP2]) + '-' + FAPconverter(elements[FAP1])\n",
    "print(len(glawi_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4102a",
   "metadata": {},
   "source": [
    "# create binary files for graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f20a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "frcowvec_categories = {'Nm': 'NOM', 'Nf': 'NOM', 'Nmp': 'NOM', 'Nfp': 'NOM', 'Nx': 'NOM', 'More': 'NOM',\n",
    "                       'Npm': 'NAM', 'Npf': 'NAM', 'Npx': 'NAM', 'Npmp': 'NAM', 'Npfp': 'NAM',\n",
    "                       'IJ': 'INT', 'Adj': 'ADJ', 'V': 'VER', 'Num': 'NUM', 'Pro': 'PRO', 'Adv': 'ADV'}\n",
    "\n",
    "def frcowvec_cat_conversion(lexeme):\n",
    "    old_cat = lexeme.split('_')[-1]\n",
    "    new_cat = frcowvec_categories.get(old_cat)\n",
    "    return lexeme.split('_')[0] + '_' + str(new_cat)\n",
    "\n",
    "frequencies = pd.read_csv('frequencies-frcowvec.csv', header=0, index_col=0)\n",
    "frequencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9fc7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'DG-families'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "output_dir = 'DG-graph-binary'\n",
    "\n",
    "for input_file in input_files:\n",
    "    fam_id = input_file.split()[0]\n",
    "    group_id = fam_id.split('-')[0]\n",
    "    H = nx.DiGraph()\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                va = elements[graph_1] + '_' + elements[cat_1]\n",
    "                vb = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if H.has_edge(va, vb) or H.has_edge(vb, va):\n",
    "                    continue\n",
    "                try:\n",
    "                    freq_a = frequencies.loc[frcowvec_cat_conversion(va)]['freq']\n",
    "                except KeyError:\n",
    "                    freq_a = 0\n",
    "                try:\n",
    "                    freq_b = frequencies.loc[frcowvec_cat_conversion(vb)]['freq']\n",
    "                except KeyError:\n",
    "                    freq_b = 0\n",
    "                H.add_node(va, label=category_shortening(elements[cat_1]), frequency=freq_a)\n",
    "                H.add_node(vb, label=category_shortening(elements[cat_2]), frequency=freq_b)\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    if (elements[graph_1], elements[graph_2]) in glawi_dict.keys():\n",
    "                        H.add_edge(va, vb, label=elements[cstr_1] + '-' + elements[cstr_2]\\\n",
    "                                  + '$' + glawi_dict.get((elements[graph_1], elements[graph_2])))\n",
    "                    else:\n",
    "                        H.add_edge(va, vb, label=elements[cstr_1] + '-' + elements[cstr_2])\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    if (elements[graph_2], elements[graph_1]) in glawi_dict.keys():\n",
    "                        H.add_edge(vb, va, label=elements[cstr_2] + '-' + elements[cstr_1]\\\n",
    "                                  + '$' + glawi_dict.get((elements[graph_2], elements[graph_1])))\n",
    "                    else:\n",
    "                        H.add_edge(vb, va, label=elements[cstr_2] + '-' + elements[cstr_1])\n",
    "                else:\n",
    "                    if (elements[graph_1], elements[graph_2]) in glawi_dict.keys():\n",
    "                        H.add_edge(va, vb, label=elements[cstr_1] + '-' + elements[cstr_2] + '_' + elements[orientation]\\\n",
    "                                  + '$' + glawi_dict.get((elements[graph_1], elements[graph_2])))\n",
    "                        H.add_edge(vb, va, label=elements[cstr_2] + '-' + elements[cstr_1] + '_' + elements[orientation]\\\n",
    "                                  + '$' + glawi_dict.get((elements[graph_2], elements[graph_1])))\n",
    "                    else:\n",
    "                        H.add_edge(va, vb, label=elements[cstr_1] + '-' + elements[cstr_2] + '_' + elements[orientation])\n",
    "                        H.add_edge(vb, va, label=elements[cstr_2] + '-' + elements[cstr_1] + '_' + elements[orientation])\n",
    "    graph_file = open(join(output_dir, fam_id), 'wb')\n",
    "    pickle.dump(H, graph_file)\n",
    "    graph_file.close()\n",
    "    print(input_file.split()[0], end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5ff92",
   "metadata": {},
   "source": [
    "# creation of formal context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'DG-graph-binary'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "input_files.sort()\n",
    "#ignored = ['F06082', 'F06086', 'F06138', 'F04553', 'F04843', 'F04879', 'F04942', 'F04945', 'F05607', 'F05702', 'F05920', 'F05956', 'F05989', 'F05963', 'F06003', 'F06020', 'F06027', 'F06030', 'F06032', 'F06038', 'F06048', 'F06049', 'F06050', 'F06067', 'F06072', 'F06085', 'F06102', 'F06127', 'F06129', 'F06139', 'F06165', 'F06167', 'F06168', 'F06188', 'F06192', 'F06197']\n",
    "ignored = ['F06513', 'F06517', 'F06569', 'F04681', 'F04999', 'F05047', 'F05148', 'F05151', 'F05959', 'F06063', 'F06314', 'F06360', 'F06404', 'F06370', 'F06423', 'F06445', 'F06452', 'F06456', 'F06458', 'F06465', 'F06476', 'F06477', 'F06478', 'F06497', 'F06072', 'F06502', 'F06533', 'F06558', 'F06129', 'F06560', 'F06600', 'F06602', 'F06603', 'F06625', 'F06629', 'F06635']\n",
    "for i in ignored:\n",
    "    try:\n",
    "        input_files.remove(i)\n",
    "    except ValueError:\n",
    "        pass\n",
    "print(len(ignored), 'ignored')\n",
    "print(len(input_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count_dict = dict()\n",
    "for graph in input_files:\n",
    "    G2 = pickle.load(open(join(input_dir, graph), 'rb'))\n",
    "    node_count_dict[graph] = len(G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5972d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_compare(e1, e2):\n",
    "    if '$' in e1['label'] and '$' not in e2['label']:\n",
    "        return e1['label'].split('$')[0] == e2['label']\n",
    "    else:\n",
    "        return e1['label'] == e2['label']\n",
    "    \n",
    "subgroup_prev = ''\n",
    "context = pd.DataFrame()\n",
    "counter = -1\n",
    "input_files_count = len(input_files)\n",
    "for subgraph in input_files:\n",
    "    subgroup_id = subgraph.split('-')[0]\n",
    "    if subgroup_id == subgroup_prev:\n",
    "        continue\n",
    "    counter += 1\n",
    "    if counter < 2000:\n",
    "        continue\n",
    "    if counter % 1000 == 0 and counter > 2000:\n",
    "        context.index = input_files\n",
    "        context.to_csv('DG-context-' + str(counter/1000) + '.csv')\n",
    "        context = pd.DataFrame()\n",
    "    G2 = pickle.load(open(join(input_dir, subgraph), 'rb'))\n",
    "    G2_node_count = node_count_dict.get(subgraph)\n",
    "    supergroup_prev = ''\n",
    "    is_subgraph = 0\n",
    "    membership = [0] * len(input_files)\n",
    "    for counter2 in range(input_files_count-1, -1, -1):\n",
    "        if membership[counter2] == 1:\n",
    "            continue\n",
    "        supergraph = input_files[counter2]\n",
    "        supergroup_id = supergraph.split('-')[0]\n",
    "        if supergroup_id == subgroup_id:\n",
    "            membership[counter2] = 1\n",
    "            continue\n",
    "        if supergroup_id == supergroup_prev:\n",
    "            membership[counter2] = membership[counter2+1]\n",
    "            continue\n",
    "        print(subgraph + ' ' + supergraph + '        ', end='\\r')\n",
    "        G1_node_count = node_count_dict.get(supergraph)\n",
    "        if G1_node_count >= G2_node_count:\n",
    "            G1 = pickle.load(open(join(input_dir, supergraph), 'rb'))\n",
    "            GM = isomorphism.DiGraphMatcher(G1, G2, node_match=lambda v1,v2: v1['label'] == v2['label'], edge_match=edge_compare)\n",
    "            if GM.subgraph_is_isomorphic():\n",
    "                membership[counter2] = 1\n",
    "#                 try:\n",
    "#                     membership_of_G1 = context[supergroup_id.replace('F', 'G')]\n",
    "#                     membership = (membership_of_G1 | membership).astype(int)\n",
    "#                 except KeyError:\n",
    "#                     pass\n",
    "        supergroup_prev = supergroup_id\n",
    "    subgroup_prev = subgroup_id\n",
    "    membership = pd.Series(membership, name=subgroup_id.replace('F', 'G'))\n",
    "    context = pd.concat([context, membership], axis=1)\n",
    "context.index = input_files\n",
    "print(context.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.index = input_files\n",
    "context.to_csv('DG-context-last.csv')\n",
    "#context.to_csv('DG-context.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe4e02",
   "metadata": {},
   "source": [
    "## joining contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dir = 'DG-contexts'\n",
    "context_files = [f for f in listdir(context_dir) if isfile(join(context_dir, f))]\n",
    "context_files.sort()\n",
    "context_join = pd.read_csv(join('DG-contexts', 'DG-context-1.0.csv'), header=0, index_col=0)\n",
    "for i in ignored:\n",
    "    if i in context_join.index:\n",
    "        context_join.drop(index=i, inplace=True)\n",
    "for c in range(1, len(context_files)):\n",
    "    new_ctx = pd.read_csv(join('DG-contexts', context_files[c]), header=0, index_col=0)\n",
    "    for i in ignored:\n",
    "        if i in new_ctx.index:\n",
    "            new_ctx.drop(index=i, inplace=True)\n",
    "    context_join = pd.concat([context_join, new_ctx], axis=1)\n",
    "print(context_join.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_join.to_csv('DG-context.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca27a6",
   "metadata": {},
   "source": [
    "# creation of poset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_without_header = 'context_without_header.csv'\n",
    "# context_with_header = pd.read_csv('DG-context.csv', header=0, index_col=0)\n",
    "# context_with_header.to_csv(context_without_header, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e585d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('java -jar AOCPosetBuilder.jar -i ' + context_without_header + ' -a HERMES -d DG-posets/maxgraph_simplified.dot -f SIMPLIFIED -z')\n",
    "#os.system('dot -Tpdf posets/families_simplified.dot -o posets/families_simplified.pdf')\n",
    "os.system('java -jar AOCPosetBuilder.jar -i ' + context_without_header + ' -a HERMES -d DG-posets/maxgraph_full.dot -f FULL -z')\n",
    "#os.system('java -jar AOCPosetBuilder.jar -i ' + context_without_header + ' -a HERMES -d posets/families_minimal.dot -f MINIMAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ea24e",
   "metadata": {},
   "source": [
    "# parents and children from each concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_with_header = pd.read_csv('DG-context.csv', header=0, index_col=0)\n",
    "col_names = context_with_header.columns\n",
    "family_ids = context_with_header.index\n",
    "\n",
    "families = [f for f in listdir('DG-families') if isfile(join('DG-families', f))]\n",
    "families_dict = dict()  # contains the representative word for a given family\n",
    "for f in families:\n",
    "    elements = f.replace('.txt', '').split()\n",
    "    families_dict[elements[0]] = elements[1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40470f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dot = 'maxgraph_full.dot'\n",
    "simplified_dot = 'maxgraph_simplified.dot'\n",
    "directory = 'DG-posets'\n",
    "out_directory = 'DG-subposets'\n",
    "L1 = nx.DiGraph()\n",
    "L2 = nx.DiGraph()\n",
    "with codecs.open(join(directory, simplified_dot), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if '->' in line:  # a line showing edges between concepts\n",
    "            elements = line.split()\n",
    "            L1.add_edge(elements[0], elements[2])   \n",
    "            L2.add_edge(elements[2], elements[0])  \n",
    "        elif 'shape' in line:  # a line describing a concept \n",
    "            L1.add_node(line.split()[0])\n",
    "            L2.add_node(line.split()[0])\n",
    "            \n",
    "families = [f for f in listdir('DG-families') if isfile(join('DG-families', f))]\n",
    "group_prev = ''\n",
    "counter = 0\n",
    "for file_name in families:\n",
    "    #if file_name != 'F01317 abducteur.txt':\n",
    "        #continue\n",
    "    family_id = file_name.split()[0]\n",
    "    group_id = family_id.split('-')[0]\n",
    "    if group_id == group_prev:\n",
    "        continue\n",
    "    group_prev = group_id\n",
    "    \n",
    "    # find vertex that contains the intended family and its parents+children\n",
    "    try:\n",
    "        object_id = family_ids.get_loc(family_id)\n",
    "    except KeyError:\n",
    "        #  ignored families (e.g. too much Npx)\n",
    "        continue\n",
    "    with codecs.open(join(directory, simplified_dot), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if ('Object ' + str(object_id) + '\\\\n') in line:\n",
    "                vertex = line.split()[0]\n",
    "                break\n",
    "    parents = nx.descendants(L1, vertex)\n",
    "    children = nx.descendants(L2, vertex)\n",
    "    selected_vertices = parents.union(children)\n",
    "    selected_vertices.add(vertex)\n",
    "    \n",
    "    # find all vertex connected to the intended family, and write to dot\n",
    "    out_file_name = 'poset_' + family_id + '_simplified' + '.dot'\n",
    "    out_file = codecs.open(join(out_directory, out_file_name), 'w')\n",
    "    with codecs.open(join(directory, simplified_dot), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'graph' in line or 'rankdir' in line or line == '}':\n",
    "                out_file.write(line)\n",
    "                continue\n",
    "            if '->' in line:\n",
    "                elements = line.split()\n",
    "                v1 = elements[0]\n",
    "                v2 = elements[2]\n",
    "                if v1 in selected_vertices and v2 in selected_vertices:\n",
    "                    out_file.write(line)\n",
    "                continue\n",
    "            vertex_id = line.split()[0]\n",
    "            concept_id = re.search('<(.*)>', line).group(1)\n",
    "            if vertex_id in selected_vertices:\n",
    "                to_be_written = line.split('|')[0] + '|'\n",
    "                attribute_string = line.split('|')[1]\n",
    "                if 'Attribute' not in attribute_string: # empty intent\n",
    "                    #to_be_written += '|'\n",
    "                    pass\n",
    "                else:\n",
    "                    attributes = attribute_string.split('\\\\n')\n",
    "                    for attribute in attributes:\n",
    "                        if attribute == '':\n",
    "                            continue\n",
    "                        to_be_written += col_names[int(attribute.split()[1])] + '\\\\n'\n",
    "                to_be_written += '|'\n",
    "                object_string = line.split('|')[2]\n",
    "                if 'Object' not in object_string: # empty extent\n",
    "                    #to_be_written += '|'\n",
    "                    pass\n",
    "                else:\n",
    "                    objects = object_string.split('\\\\n')\n",
    "                    for obj in objects:\n",
    "                        if obj == '' or '}' in obj:\n",
    "                            continue\n",
    "                        to_be_written += families_dict[family_ids[int(obj.split()[1])]] + '\\\\n'\n",
    "                to_be_written += '}\"];\\n'\n",
    "                to_be_written = re.sub('\\(I.*\\)\\|', concept_id + '|', to_be_written)\n",
    "                to_be_written = to_be_written.replace(',fillcolor=orange', '').replace(',fillcolor=lightblue', '')\n",
    "                if vertex_id == vertex:\n",
    "                    to_be_written = to_be_written.replace('style=filled', 'style=filled,fillcolor=orange')\n",
    "                out_file.write(to_be_written)\n",
    "    out_file.close()\n",
    "    counter += 1\n",
    "    print(group_id, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ad927",
   "metadata": {},
   "source": [
    "# create fully-oriented graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'DG-graph-binary'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directed_edge_count = dict()\n",
    "for graph in input_files:\n",
    "    G = pickle.load(open(join(input_dir, graph), 'rb'))\n",
    "    for e in G.edges(data=True):\n",
    "        if 'indirect' in e[2]['label'].split('$')[0] or 'NA' in e[2]['label'].split('$')[0]:\n",
    "            continue\n",
    "        label = G.nodes[e[0]]['label'] + '>' + e[2]['label'].split('$')[0] + '>' + G.nodes[e[1]]['label']\n",
    "        if label not in directed_edge_count:\n",
    "            directed_edge_count[label] = 1\n",
    "        else:\n",
    "            directed_edge_count[label] += 1\n",
    "    print(graph, end='\\r')\n",
    "# for c in directed_edge_count:\n",
    "#     print(c, directed_edge_count[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ff58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'DG-graph-oriented'\n",
    "original_graph_viz_dir = 'DG-graph'\n",
    "new_directed_edge_count = directed_edge_count.copy()  # taking into account directed edges that are originally non-directed\n",
    "for graph in input_files:\n",
    "    L = nx.DiGraph()\n",
    "    G = pickle.load(open(join(input_dir, graph), 'rb'))\n",
    "    for n in G.nodes(data=True):\n",
    "        L.add_node(n[0], label=n[0].split('_')[0] + '\\n' + n[1]['label'])\n",
    "    for e in G.edges(data=True):\n",
    "#         print(e)\n",
    "        if 'indirect' in e[2]['label'].split('$')[0] or 'NA' in e[2]['label'].split('$')[0]:\n",
    "            label = e[2]['label'].split('$')[0].replace('_indirect', '').replace('_NA', '')\n",
    "            if L.has_edge(e[1], e[0]) or L.has_edge(e[0], e[1]):\n",
    "                continue\n",
    "            patterns = label.split('-')\n",
    "            label1 = G.nodes[e[0]]['label'] + '>' + patterns[0] + '-' + patterns[1] + '>' + G.nodes[e[1]]['label']\n",
    "            label2 = G.nodes[e[1]]['label'] + '>' + patterns[1] + '-' + patterns[0] + '>' + G.nodes[e[0]]['label']\n",
    "#             print(label1, directed_edge_count.get(label1, 0))\n",
    "#             print(label2, directed_edge_count.get(label2, 0))\n",
    "            if directed_edge_count.get(label1, 0) == directed_edge_count.get(label2, 0):\n",
    "                if patterns[0] < patterns[1]:\n",
    "                    L.add_edge(e[0], e[1], label=patterns[0] + '-' + patterns[1])\n",
    "                    try:\n",
    "                        new_directed_edge_count[label1] += 1\n",
    "                    except KeyError:\n",
    "                        new_directed_edge_count[label1] = 1\n",
    "#                     print('add', e[0], e[1])\n",
    "                else:\n",
    "                    L.add_edge(e[1], e[0], label=patterns[1] + '-' + patterns[0])\n",
    "                    try:\n",
    "                        new_directed_edge_count[label2] += 1\n",
    "                    except KeyError:\n",
    "                        new_directed_edge_count[label2] = 1\n",
    "#                     print('add', e[1], e[0])\n",
    "            elif directed_edge_count.get(label1, 0) > directed_edge_count.get(label2, 0):\n",
    "                L.add_edge(e[0], e[1], label=patterns[0] + '-' + patterns[1])\n",
    "                new_directed_edge_count[label1] += 1\n",
    "#                 print('add', e[0], e[1])\n",
    "            else:\n",
    "                L.add_edge(e[1], e[0], label=patterns[1] + '-' + patterns[0])\n",
    "                new_directed_edge_count[label2] += 1\n",
    "#                 print('add', e[1], e[0])\n",
    "        else:\n",
    "            L.add_edge(e[0], e[1], label=e[2]['label'].split('$')[0])\n",
    "    filename = glob.glob(join(original_graph_viz_dir, graph + '*'))[0]\n",
    "    write_dot(L, filename.replace(original_graph_viz_dir, output_dir))\n",
    "    print(graph, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98daead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = codecs.open('DG_edge_count_directed.txt', 'w')\n",
    "for c in directed_edge_count:\n",
    "    out_file.write(c + ',' + str(directed_edge_count[c]) + '\\n')\n",
    "out_file.close()\n",
    "out_file = codecs.open('DG_edge_count_all.txt', 'w')\n",
    "for c in new_directed_edge_count:\n",
    "    out_file.write(c + ',' + str(new_directed_edge_count[c]) + '\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368c78d",
   "metadata": {},
   "source": [
    "# generate tree for each family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71cb7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'DG-graph-binary'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f753c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "directed_edge_count = dict()\n",
    "with codecs.open('DG_edge_count_directed.txt', 'r', encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "        elements = line.replace('\\n', '').split(',')\n",
    "        directed_edge_count[elements[0]] = int(elements[1])\n",
    "new_directed_edge_count = dict()\n",
    "with codecs.open('DG_edge_count_all.txt', 'r', encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "        elements = line.replace('\\n', '').split(',')\n",
    "        new_directed_edge_count[elements[0]] = int(elements[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465380cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F08068-5283\r"
     ]
    }
   ],
   "source": [
    "output_dir = 'DG-tree'\n",
    "original_graph_viz_dir = 'DG-graph'\n",
    "new_directed_edge_count = directed_edge_count.copy()  # taking into account directed edges that are originally non-directed\n",
    "counter = -1\n",
    "for graph in input_files:\n",
    "#     if graph != 'F00020':\n",
    "#         continue\n",
    "    counter += 1\n",
    "#     if counter < 2582:\n",
    "#         continue\n",
    "    L = nx.DiGraph()\n",
    "    G = pickle.load(open(join(input_dir, graph), 'rb'))\n",
    "    for n in G.nodes(data=True):\n",
    "        L.add_node(n[0], label=n[0].split('_')[0] + '\\n' + n[1]['label'])\n",
    "    for e in G.edges(data=True):\n",
    "#         print(e)\n",
    "        if 'indirect' in e[2]['label'].split('$')[0] or 'NA' in e[2]['label'].split('$')[0]:\n",
    "            label = e[2]['label'].split('$')[0].replace('_indirect', '').replace('_NA', '')\n",
    "            if L.has_edge(e[1], e[0]) or L.has_edge(e[0], e[1]):\n",
    "                continue\n",
    "            patterns = label.split('-')\n",
    "            label1 = G.nodes[e[0]]['label'] + '>' + patterns[0] + '-' + patterns[1] + '>' + G.nodes[e[1]]['label']\n",
    "            label2 = G.nodes[e[1]]['label'] + '>' + patterns[1] + '-' + patterns[0] + '>' + G.nodes[e[0]]['label']\n",
    "#             print(label1, directed_edge_count.get(label1, 0))\n",
    "#             print(label2, directed_edge_count.get(label2, 0))\n",
    "            if directed_edge_count.get(label1, 0) == directed_edge_count.get(label2, 0):\n",
    "                if patterns[0] < patterns[1]:\n",
    "                    w = new_directed_edge_count.get(label1, 0)\n",
    "                    L.add_edge(e[0], e[1], weight=w, label=patterns[0] + '-' + patterns[1])\n",
    "#                     print('add', e[0], e[1])\n",
    "                else:\n",
    "                    w = new_directed_edge_count.get(label2, 0)\n",
    "                    L.add_edge(e[1], e[0], weight=w, label=patterns[1] + '-' + patterns[0])\n",
    "#                     print('add', e[1], e[0])\n",
    "            elif directed_edge_count.get(label1, 0) > directed_edge_count.get(label2, 0):\n",
    "                w = new_directed_edge_count.get(label1, 0)\n",
    "                L.add_edge(e[0], e[1], weight=w, label=patterns[0] + '-' + patterns[1])\n",
    "#                 print('add', e[0], e[1])\n",
    "            else:\n",
    "                w = new_directed_edge_count.get(label2, 0)\n",
    "                L.add_edge(e[1], e[0], weight=w, label=patterns[1] + '-' + patterns[0])\n",
    "#                 print('add', e[1], e[0])\n",
    "        else:\n",
    "            edge_type = G.nodes[e[0]]['label'] + '>' + e[2]['label'].split('$')[0] + '>' + G.nodes[e[1]]['label']\n",
    "            w = new_directed_edge_count.get(edge_type, 0)\n",
    "            L.add_edge(e[0], e[1], weight=w, label=e[2]['label'].split('$')[0])\n",
    "    #T = nx.algorithms.tree.branchings.maximum_spanning_arborescence(L, attr='weight', default=0, preserve_attrs=True)\n",
    "    edmonds = nx.algorithms.tree.branchings.Edmonds(L)\n",
    "    T = edmonds.find_optimum(preserve_attrs=True)\n",
    "    filename = glob.glob(join(original_graph_viz_dir, graph + '*'))[0]\n",
    "    write_dot(T, filename.replace(original_graph_viz_dir, output_dir))\n",
    "    print(graph, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e802ca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
