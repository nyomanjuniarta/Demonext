{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3da0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from networkx.algorithms import isomorphism\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "from utils import printProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17593fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dir = 'DG-graph-binary'\n",
    "input_files = [f for f in listdir(binary_dir) if isfile(join(binary_dir, f))]\n",
    "input_files.sort()\n",
    "print(len(input_files), 'families')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "glawi_constructions = list()\n",
    "with codecs.open('glawi-constructions.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        glawi_constructions.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(pattern, word):  # pattern = 'preXisation', word = 'precognisation' => True\n",
    "    if pattern == 'X':\n",
    "        return True\n",
    "    counter = 0\n",
    "    try:\n",
    "        for c in pattern:\n",
    "            if c == 'X':\n",
    "                break\n",
    "            if c != word[counter]:\n",
    "                return False\n",
    "            counter += 1\n",
    "        counter = -1\n",
    "        while True:\n",
    "            if pattern[counter] == 'X':\n",
    "                break\n",
    "            if pattern[counter] != word[counter]:\n",
    "                return False\n",
    "            counter -= 1\n",
    "    except IndexError:  # Xtractif & actif\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9aedb3",
   "metadata": {},
   "source": [
    "# generate propositions and their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cow_dict = dict()\n",
    "with codecs.open('frequencies-frcowvec.csv', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if '_PUN' in line:\n",
    "            continue\n",
    "        elements = line.replace('\"', '').strip('\\n').split(',')\n",
    "        if elements[-1] == 'freq':\n",
    "            continue\n",
    "        lexeme_and_cat = ','.join(elements[0:-1])\n",
    "        lexeme = '_'.join(lexeme_and_cat.split('_')[0:-1])\n",
    "        try:\n",
    "            if cow_dict[lexeme] < int(elements[-1]):\n",
    "                cow_dict[lexeme] = int(elements[-1])\n",
    "        except KeyError:\n",
    "            cow_dict[lexeme] = int(elements[-1])\n",
    "print(len(cow_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cee59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexemes_in_cow = set()\n",
    "# with codecs.open('frequencies-frcowvec-filtered.csv', 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         elements = line.strip('\\n').split(',')\n",
    "#         if elements[1] == 'freq':\n",
    "#             continue\n",
    "#         lexemes_in_cow.add(elements[0].split('_')[0])\n",
    "# print(len(lexemes_in_cow), 'lexemes in cow')\n",
    "\n",
    "lexemes_in_demTable = set()\n",
    "with codecs.open('lexemes.csv', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        elements = line.split('\\t')\n",
    "        if elements[0] == 'lid':\n",
    "            continue\n",
    "        lexemes_in_demTable.add(elements[2])\n",
    "print(len(lexemes_in_demTable), 'lexemes in Demonette\\'s table of lexemes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lexemes_and_freq(nodes):\n",
    "    return_list = list()\n",
    "    existing_lexemes = set()\n",
    "    generated = set()\n",
    "    for n in nodes:\n",
    "        existing_lexemes.add(n.split('_')[0])\n",
    "    for lexeme in existing_lexemes:\n",
    "        best_const_length = -1\n",
    "        best_const = ''\n",
    "        for c in glawi_constructions:\n",
    "            const1 = c.split('-')[0]\n",
    "            if len(const1) > best_const_length and match(const1, lexeme):\n",
    "                best_const_length = len(const1)\n",
    "                best_const = const1\n",
    "        for c in glawi_constructions:\n",
    "            [const1, const2] = c.split('-')\n",
    "            if const1 != best_const:\n",
    "                continue\n",
    "            [prefix, postfix] = const1.split('X')\n",
    "            stem = lexeme.replace(prefix, '', 1)\n",
    "            if postfix:  # if not empty\n",
    "                stem = ''.join(stem.rsplit(postfix, 1))\n",
    "            new_lexeme = const2.replace('X', stem)\n",
    "            if new_lexeme in generated or new_lexeme in existing_lexemes:\n",
    "                continue\n",
    "            if new_lexeme in lexemes_in_demTable or new_lexeme in cow_dict:\n",
    "                return_list.append((new_lexeme, cow_dict.get(new_lexeme, 0)))\n",
    "                generated.add(new_lexeme)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d222c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = codecs.open('DG_propositions_and_freq.txt', 'w', encoding='utf-8')\n",
    "# counter_file = -1\n",
    "proposed_lexemes = set()\n",
    "for input_file in input_files:\n",
    "    print(input_file, end='\\r')\n",
    "#     counter_file += 1\n",
    "#     if counter_file > 20:\n",
    "#         break\n",
    "    G = pickle.load(open(join(binary_dir, input_file), 'rb'))\n",
    "    prop_and_freq = generate_lexemes_and_freq(G.nodes())\n",
    "    prop_and_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "    output_file.write(input_file + '\\t' + str(prop_and_freq) + '\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0dba79",
   "metadata": {},
   "source": [
    "# generate propositions and their cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c48639",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexemes_in_bow = set()\n",
    "with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lexeme = line.split()[0].split('_')[0]\n",
    "        lexemes_in_bow.add(lexeme)\n",
    "print(len(lexemes_in_bow), 'lexemes have distribution vectors')\n",
    "\n",
    "lexemes_in_demonette_families = set()\n",
    "for file in input_files:\n",
    "    G = pickle.load(open(join(binary_dir, file), 'rb'))\n",
    "    for n in G.nodes():\n",
    "        lexemes_in_demonette_families.add(n.split('_')[0])\n",
    "print(len(lexemes_in_demonette_families), 'lexemes in Demonette')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f3b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lexemes(nodes):\n",
    "    return_set = set()\n",
    "    existing_lexemes = set()\n",
    "    for n in nodes:\n",
    "        existing_lexemes.add(n.split('_')[0])\n",
    "    for lexeme in existing_lexemes:\n",
    "        best_const_length = -1\n",
    "        best_const = ''\n",
    "        for c in glawi_constructions:\n",
    "            const1 = c.split('-')[0]\n",
    "            if len(const1) > best_const_length and match(const1, lexeme):\n",
    "                best_const_length = len(const1)\n",
    "                best_const = const1\n",
    "        for c in glawi_constructions:\n",
    "            [const1, const2] = c.split('-')\n",
    "            if const1 != best_const:\n",
    "                continue\n",
    "            [prefix, postfix] = const1.split('X')\n",
    "            stem = lexeme.replace(prefix, '', 1)\n",
    "            if postfix:  # if not empty\n",
    "                stem = ''.join(stem.rsplit(postfix, 1))\n",
    "            new_lexeme = const2.replace('X', stem)\n",
    "            if new_lexeme in return_set or new_lexeme in existing_lexemes:\n",
    "                continue\n",
    "            if new_lexeme in lexemes_in_bow:\n",
    "                return_set.add(new_lexeme)\n",
    "    return return_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff229a",
   "metadata": {},
   "source": [
    "## generate propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = codecs.open('DG_propositions.txt', 'w', encoding='utf-8')\n",
    "# counter_file = -1\n",
    "proposed_lexemes = set()\n",
    "for input_file in input_files:\n",
    "    print(input_file, end='\\r')\n",
    "#     counter_file += 1\n",
    "#     if counter_file < 82:\n",
    "#         continue\n",
    "    G = pickle.load(open(join(binary_dir, input_file), 'rb'))\n",
    "    propositions = generate_lexemes(G.nodes())\n",
    "    proposed_lexemes.update(propositions)\n",
    "    output_file.write(input_file + '\\t' + str(propositions) + '\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(proposed_lexemes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f04d6",
   "metadata": {},
   "source": [
    "## max and average of cosine similarity in each family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662dac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dict = dict()\n",
    "counter = 0\n",
    "with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lexeme = line.split()[0].split('_')[0]\n",
    "        if lexeme in lexemes_in_demonette_families: #  or lexeme in proposed_lexemes:\n",
    "            v = np.array(list(map(lambda x: float(x), line.split()[1:])))\n",
    "            vector_dict[lexeme] = v\n",
    "        print(counter, end='\\r')\n",
    "        counter += 1\n",
    "print(counter, 'vectors in bow')\n",
    "print(len(vector_dict), 'vectors kept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fae8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = codecs.open('DG_family_cosine.txt', 'w', encoding='utf-8')\n",
    "output_file.write('familyID\\tmax_cosine\\tavg_cosine\\tlexemes\\n')\n",
    "for file in input_files:\n",
    "    G = pickle.load(open(join(binary_dir, file), 'rb'))\n",
    "    max_cos = -3\n",
    "    comparison_counter = 0\n",
    "    total_cosine = 0\n",
    "    nodes = list(G.nodes())\n",
    "    for n1 in range(0, len(nodes) - 1):\n",
    "        for n2 in range(n1 + 1, len(nodes) - 1):\n",
    "            try:\n",
    "                lex1 = nodes[n1].split('_')[0]\n",
    "                lex2 = nodes[n2].split('_')[0]\n",
    "                if lex1 == lex2:\n",
    "                    continue\n",
    "                vec1 = vector_dict[lex1]\n",
    "                vec2 = vector_dict[lex2]\n",
    "                cosine_similarity = dot(vec1, vec2)/(norm(vec1)*norm(vec2))\n",
    "                if cosine_similarity > max_cos:\n",
    "                    max_cos = cosine_similarity\n",
    "                total_cosine += cosine_similarity\n",
    "                comparison_counter += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "    if comparison_counter == 0:\n",
    "        output_file.write(file + '\\t?\\t0\\t' + str(nodes) + '\\n')\n",
    "    else:\n",
    "        output_file.write(file + '\\t' + str(round(max_cos, 2)) + '\\t' + str(round(total_cosine/comparison_counter, 2))\\\n",
    "                      + '\\t' + str(nodes) + '\\n')\n",
    "    print(file, end='\\r')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f50e5d",
   "metadata": {},
   "source": [
    "## calculate cosine for each proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_lexemes = set()\n",
    "with codecs.open('DG_propositions.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        elements = line.split('\\t')\n",
    "        propositions = eval(elements[1])\n",
    "        proposed_lexemes.update(propositions)\n",
    "        print(elements[0], end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dict = dict()\n",
    "counter = 0\n",
    "with codecs.open('lemma-A-pos-bow.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lexeme = line.split()[0].split('_')[0]\n",
    "        if lexeme in lexemes_in_demonette_families or lexeme in proposed_lexemes:\n",
    "            v = np.array(list(map(lambda x: float(x), line.split()[1:])))\n",
    "            vector_dict[lexeme] = v\n",
    "        print(counter, end='\\r')\n",
    "        counter += 1\n",
    "print(counter, 'vectors in bow')\n",
    "print(len(vector_dict), 'vectors kept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b67cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = codecs.open('DG_propositions_and_cosine.txt', 'w', encoding='utf-8')\n",
    "with codecs.open('DG_propositions.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        elements = line.split('\\t')\n",
    "        output_file.write(elements[0] + '\\t')\n",
    "        propositions = eval(elements[1])\n",
    "        prop_and_cos = list()\n",
    "        for p in propositions:\n",
    "            total_cos = 0\n",
    "            count_cos = 0\n",
    "            max_cos = -2\n",
    "            vec1 = vector_dict[p]\n",
    "            G = pickle.load(open(join(binary_dir, elements[0]), 'rb'))\n",
    "            for n in G.nodes():\n",
    "                try:\n",
    "                    vec2 = vector_dict[n.split('_')[0]]\n",
    "                    cosine_similarity = dot(vec1, vec2)/(norm(vec1)*norm(vec2))\n",
    "                    if cosine_similarity > max_cos:\n",
    "                        max_cos = cosine_similarity\n",
    "                    total_cos += cosine_similarity\n",
    "                    count_cos += 1\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if count_cos == 0:\n",
    "                prop_and_cos.append((p, 0, 0))\n",
    "            else:\n",
    "                prop_and_cos.append((p, round(max_cos, 2), round(total_cos/count_cos, 2)))\n",
    "        if len(prop_and_cos) > 0:\n",
    "            prop_and_cos.sort(key=lambda x:x[1], reverse=True)\n",
    "        output_file.write(str(prop_and_cos) + '\\n')\n",
    "        print(elements[0], end='\\r')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e52ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
