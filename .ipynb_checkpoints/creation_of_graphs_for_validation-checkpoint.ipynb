{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e56cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import networkx as nx\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from difflib import SequenceMatcher\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "from networkx.algorithms import isomorphism\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from utils import printProgressBar\n",
    "\n",
    "# column number\n",
    "rid = 0\n",
    "fid = 1\n",
    "lid_1 = 2\n",
    "graph_1 = 3\n",
    "ori_graph_1 = 4\n",
    "lid_2 = 5\n",
    "graph_2 = 6\n",
    "ori_graph_2 = 7\n",
    "cat_1 = 8\n",
    "ori_cat_1 = 9\n",
    "cat_2 = 10\n",
    "ori_cat_2 = 11\n",
    "ori_cple = 12\n",
    "type_cstr_1 = 13\n",
    "cstr_1 = 14\n",
    "ori_cstr_1 = 15\n",
    "type_cstr_2 = 16\n",
    "cstr_2 = 17\n",
    "ori_cstr_2 = 18\n",
    "complexite = 19\n",
    "ori_complexite = 20\n",
    "orientation = 21\n",
    "ori_orientation = 22\n",
    "semty_1 = 23\n",
    "ori_semty_1 = 24\n",
    "semty_2 = 25\n",
    "ori_semty_2 = 26\n",
    "sous_semty_1 = 27\n",
    "sous_semty_2 = 28\n",
    "ori_sous_semty_1 = 29\n",
    "ori_sous_semty_2 = 30\n",
    "semtyrss_1 = 31\n",
    "semtyrss_2 = 32\n",
    "ori_semtyrss_1 = 33\n",
    "ori_semtyrss_2 = 34\n",
    "rel_sem_n1 = 35\n",
    "rel_sem_n2 = 36\n",
    "ori_relsem = 37\n",
    "def_conc = 38\n",
    "ori_def_conc = 39\n",
    "def_abs = 40\n",
    "ori_def_abs = 41\n",
    "commentaires = 42\n",
    "fichier_origine = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aac619e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_shortening(cat):\n",
    "    if cat != 'Num' and cat[0] == 'N':\n",
    "        return 'N'\n",
    "    return cat\n",
    "\n",
    "def origine(origine_input):\n",
    "    if origine_input == 'CV':\n",
    "        return 'C'\n",
    "    return origine_input\n",
    "\n",
    "def generate_png(target_folder):\n",
    "    dot_files = [f for f in listdir(target_folder) if isfile(join(target_folder, f)) and '.dot' in f]\n",
    "    counter = 0\n",
    "    for dot_file in dot_files:\n",
    "        os.system('dot -Tpng \"' + join(target_folder, dot_file) + '\" -o \"' + join(target_folder, dot_file.replace('.dot', '.png')) + '\"')\n",
    "        counter += 1\n",
    "        printProgressBar(counter, len(dot_files), prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff05447c",
   "metadata": {},
   "source": [
    "# spurious lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f2eafb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(511, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_folder = 'families'\n",
    "spurious_folder = 'web_spurious'\n",
    "model_folder = 'web_reference'\n",
    "df = pd.read_excel('concept_comparison_maxgraph_spurious.xlsx')\n",
    "selected_rows = df[df['spurious_node_freq'] < 1]\n",
    "selected_rows = selected_rows[selected_rows['parent_node_count'] > 2]\n",
    "selected_rows.to_excel('spurious.xlsx', index=False)\n",
    "selected_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59af39be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.00% complete\n",
      "Progress: |██████████████████████████████████████████████████| 100.00% complete\n"
     ]
    }
   ],
   "source": [
    "def create_graph(output_folder, in_dot_file_name, out_dot_file_name, spurious=''):\n",
    "    G = pickle.load(open(join('graph_binary', in_dot_file_name.split()[0]), 'rb'))\n",
    "    H = nx.DiGraph()\n",
    "    with codecs.open(join(family_folder, in_dot_file_name.replace('.dot', '.txt')), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                v1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "                v2 = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if H.has_edge(v1, v2) or H.has_edge(v2, v1):\n",
    "                    continue\n",
    "                H.add_node(v1, label=elements[graph_1] + '\\n' + category_shortening(elements[cat_1]) + ', ' + str(G.nodes[v1]['frequency']))\n",
    "                H.add_node(v2, label=elements[graph_2] + '\\n' + category_shortening(elements[cat_2]) + ', ' + str(G.nodes[v2]['frequency']))\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    edge_type = origine(elements[fichier_origine]) + ': ' + elements[cstr_1] + '-' + elements[cstr_2]\n",
    "                    H.add_edge(v1, v2, label=edge_type, style='')\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    edge_type = origine(elements[fichier_origine]) + ': ' + elements[cstr_2] + '-' + elements[cstr_1]\n",
    "                    H.add_edge(v2, v1, label=edge_type, style='')\n",
    "                elif elements[orientation] == 'indirect':\n",
    "                    sorted_lex = sorted([v1, v2])\n",
    "                    sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    edge_type = origine(elements[fichier_origine]) + ': ' + sorted_cstr[0] + '-' + sorted_cstr[1]\n",
    "                    H.add_edge(sorted_lex[0], sorted_lex[1], dir='none', style='dotted', label=edge_type)\n",
    "                elif elements[orientation] == 'NA':\n",
    "                    sorted_lex = sorted([v1, v2])\n",
    "                    sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    edge_type = origine(elements[fichier_origine]) + ': ' + sorted_cstr[0] + '-' + sorted_cstr[1]\n",
    "                    H.add_edge(sorted_lex[0], sorted_lex[1], dir='none', style='dashed', label=edge_type)\n",
    "                else:\n",
    "                    print(input_file, elements[orientation])\n",
    "    if spurious == '':\n",
    "        write_dot(H, join(output_folder, out_dot_file_name))\n",
    "    else:\n",
    "        spurious_nodes = spurious.split(', ')\n",
    "        for spurious_node in spurious_nodes:\n",
    "            for in_edge in H.in_edges(spurious_node):\n",
    "                H.edges[in_edge]['color'] = 'red'\n",
    "                H.edges[in_edge]['fontcolor'] = 'red'\n",
    "            for out_edge in H.out_edges(spurious_node):\n",
    "                H.edges[out_edge]['color'] = 'red'\n",
    "                H.edges[out_edge]['fontcolor'] = 'red'\n",
    "            H.nodes[spurious_node]['color'] = 'red'\n",
    "            H.nodes[spurious_node]['fontcolor'] = 'red'\n",
    "        write_dot(H, join(output_folder, out_dot_file_name))\n",
    "\n",
    "counter = 0\n",
    "for index, row in selected_rows.iterrows():\n",
    "    create_graph(spurious_folder, row['child'], row['parent'].split()[0] + '_' + row['child'].split()[0] + '.dot', row['spurious_node'])\n",
    "    counter += 1\n",
    "    printProgressBar(counter, selected_rows.shape[0], prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)\n",
    "\n",
    "list_of_parents = selected_rows['parent'].unique()\n",
    "counter = 0\n",
    "for parent in list_of_parents:\n",
    "    create_graph(model_folder, parent, parent)\n",
    "    counter += 1\n",
    "    printProgressBar(counter, len(list_of_parents), prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba96fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.00% complete\n"
     ]
    }
   ],
   "source": [
    "generate_png(spurious_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8df685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.00% complete\n"
     ]
    }
   ],
   "source": [
    "generate_png(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c11f86",
   "metadata": {},
   "source": [
    "# missing lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_folder = 'families'\n",
    "missing_folder = 'missing_lexemes'\n",
    "model_folder = 'graph_visualization_web'\n",
    "df = pd.read_excel('concept_comparison_maxgraph_missing.xlsx')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ecc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = pd.read_csv('frequencies-frcowvec-filtered.csv', header=0, index_col=0)\n",
    "frequencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c594f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "frcowvec_categories = {'Nm': 'NOM', 'Nf': 'NOM', 'Nmp': 'NOM', 'Nfp': 'NOM', 'Nx': 'NOM', 'More': 'NOM',\n",
    "                       'Npm': 'NAM', 'Npf': 'NAM', 'Npx': 'NAM', 'Npmp': 'NAM', 'Npfp': 'NAM',\n",
    "                       'Adj': 'ADJ', 'V': 'VER', 'Num': 'NUM', 'Pro': 'PRO', 'Adv': 'ADV'}\n",
    "\n",
    "def frcowvec_cat_conversion(lexeme):\n",
    "    old_cat = lexeme.split('_')[-1]\n",
    "    new_cat = frcowvec_categories.get(old_cat)\n",
    "    return lexeme.split('_')[0] + '_' + new_cat\n",
    "\n",
    "def get_frequency(lexeme):\n",
    "    if '??' in lexeme:\n",
    "        return 0\n",
    "    else:\n",
    "        try:\n",
    "            freq = frequencies.loc[frcowvec_cat_conversion(lexeme)]['freq']\n",
    "            return freq\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "def guess_missing_lexeme(in_a1, in_a2, in_b1):\n",
    "    # input_str ex : \"micocoulier_Nm : micocoule_Nf = cotonéaster_Nm : ?, micocoulier_Nm : micocouleraie_Nf = cotonéaster_Nm : ?\"\n",
    "    a1 = '{' + in_a1.split('_')[0] + '}'\n",
    "    a2 = '{' + in_a2.split('_')[0] + '}'\n",
    "    b1 = '{' + in_b1.split('_')[0] + '}'\n",
    "    a2_cat = in_a2.split('_')[1]\n",
    "    match = SequenceMatcher(None, a1, a2).find_longest_match(0, len(a1), 0, len(a2))\n",
    "    common = a1[match.a:match.a+match.size]\n",
    "    a1_suffix = a1.replace(common, '')\n",
    "    a2_suffix = a2.replace(common, '')\n",
    "    b2 = b1.replace(a1_suffix, a2_suffix)\n",
    "    if b2 == b1 and a1 != a2:\n",
    "        return '??'\n",
    "    else:\n",
    "        return b2.replace('{', '').replace('}', '') + '_' + a2_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_for_missing_lexemes(parent, child, output_folder, output_filename):\n",
    "    H = nx.DiGraph()\n",
    "    if parent == '':\n",
    "        to_be_generated = child\n",
    "    else:\n",
    "        to_be_generated = parent\n",
    "    G = pickle.load(open(join('graph_binary', to_be_generated.split()[0]), 'rb'))\n",
    "    with codecs.open(join('families', to_be_generated.replace('dot', 'txt').replace(' **', '')), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                v1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "                v2 = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if H.has_edge(v1, v2) or H.has_edge(v2, v1):\n",
    "                    continue\n",
    "                H.add_node(v1, label=elements[graph_1] + '\\n' + category_shortening(elements[cat_1]) + ', ' + str(G.nodes[v1]['frequency']))\n",
    "                H.add_node(v2, label=elements[graph_2] + '\\n' + category_shortening(elements[cat_2]) + ', ' + str(G.nodes[v2]['frequency']))\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    edge_type = origine(elements[fichier_origine]) + ': ' + elements[cstr_1] + '-' + elements[cstr_2]\n",
    "                    H.add_edge(v1, v2, label=edge_type, style='')\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    edge_type = origine(elements[fichier_origine]) + ': ' + elements[cstr_2] + '-' + elements[cstr_1]\n",
    "                    H.add_edge(v2, v1, label=edge_type, style='')\n",
    "                elif elements[orientation] == 'indirect':\n",
    "                    sorted_lex = sorted([v1, v2])\n",
    "                    sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    edge_type = origine(elements[fichier_origine]) + ': ' + sorted_cstr[0] + '-' + sorted_cstr[1]\n",
    "                    H.add_edge(sorted_lex[0], sorted_lex[1], dir='none', style='dotted', label=edge_type)\n",
    "                elif elements[orientation] == 'NA':\n",
    "                    sorted_lex = sorted([v1, v2])\n",
    "                    sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    edge_type = origine(elements[fichier_origine]) + ': ' + sorted_cstr[0] + '-' + sorted_cstr[1]\n",
    "                    H.add_edge(sorted_lex[0], sorted_lex[1], dir='none', style='dashed', label=edge_type)\n",
    "    if parent == '':\n",
    "        write_dot(H, join(output_folder, output_filename))\n",
    "        return\n",
    "    if '**' in parent:\n",
    "        G_parent = pickle.load(open(join('graph_binary', parent.split()[0]), 'rb'))\n",
    "        G_child = pickle.load(open(join('graph_binary', child.split()[0]), 'rb'))\n",
    "        GM = isomorphism.DiGraphMatcher(G_child, G_parent, node_match=lambda v1,v2: v1['label'] == v2['label'], edge_match=lambda e1,e2: e1['label'] == e2['label'])\n",
    "        node_diff = set()\n",
    "        for subgraph in GM.subgraph_isomorphisms_iter():\n",
    "            node_diff = G_child.nodes - subgraph\n",
    "        #print('subgraph', subgraph)\n",
    "        #print('node_diff', node_diff)\n",
    "        for n in node_diff:\n",
    "            #print('in:', list(G_child.in_edges(n, data=True)))\n",
    "            #print('out:', list(G_child.out_edges(n, data=True)))\n",
    "            new_lexeme = n\n",
    "            if len(G_child.in_edges(n)) > 0:\n",
    "                one_origin = list(G_child.in_edges(n))[0][0]\n",
    "                try:\n",
    "                    new_lexeme = guess_missing_lexeme(one_origin, n, subgraph.get(one_origin))\n",
    "                except TypeError:\n",
    "                    pass\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "            else:\n",
    "                one_derived = list(G_child.out_edges(n))[0][1]\n",
    "                try:\n",
    "                    new_lexeme = guess_missing_lexeme(one_derived, n, subgraph.get(one_derived))\n",
    "                except TypeError:\n",
    "                    pass\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "            new_node_freq = get_frequency(new_lexeme)\n",
    "            if new_lexeme == '??':\n",
    "                new_lexeme = new_lexeme + '_' + n\n",
    "                H.add_node(new_lexeme, label='?? \\n' + category_shortening(n.split('_')[1]) + ', ' + str(new_node_freq), fontcolor='blue', color='blue')\n",
    "            else:\n",
    "                H.add_node(new_lexeme, label=new_lexeme.split('_')[0] + '\\n' + category_shortening(n.split('_')[1]) + ', ' + str(new_node_freq), fontcolor='blue', color='blue')\n",
    "            for i in list(G_child.in_edges(n, data=True)):\n",
    "                if 'NA' in i[2].get('label') and subgraph.get(i[0]) is not None:\n",
    "                    H.add_edge(subgraph.get(i[0]), new_lexeme, label=i[2].get('label').split('_')[0], style='dashed', dir='none', fontcolor='blue', color='blue')\n",
    "                elif 'indirect' in i[2].get('label') and subgraph.get(i[0]) is not None:\n",
    "                    H.add_edge(subgraph.get(i[0]), new_lexeme, label=i[2].get('label').split('_')[0], style='dotted', dir='none', fontcolor='blue', color='blue')\n",
    "                elif subgraph.get(i[0]) is not None:\n",
    "                    H.add_edge(subgraph.get(i[0]), new_lexeme, label=i[2].get('label'), fontcolor='blue', color='blue')\n",
    "            for o in list(G_child.out_edges(n, data=True)):\n",
    "                if 'NA' in o[2].get('label') or 'indirect' in o[2].get('label'):\n",
    "                    continue\n",
    "                if subgraph.get(o[1]) is not None:\n",
    "                    H.add_edge(new_lexeme, subgraph.get(o[1]), label=o[2].get('label'), fontcolor='blue', color='blue')\n",
    "        write_dot(H, join(output_folder, output_filename))\n",
    "    else:\n",
    "        pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df517f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_child = df['child'].unique()\n",
    "counter = 0\n",
    "for child in list_of_child:\n",
    "    create_graph_for_missing_lexemes('', child, model_folder, child)\n",
    "    counter += 1\n",
    "    printProgressBar(counter, len(list_of_child), prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c634754",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for index, row in df.iterrows():\n",
    "    create_graph_for_missing_lexemes(row['parent'], row['child'], missing_folder, row['parent'].split()[0] + '_' + row['child'].split()[0] + '.dot')\n",
    "    counter += 1\n",
    "    printProgressBar(counter, df.shape[0], prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_png(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_png(missing_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebbf30",
   "metadata": {},
   "source": [
    "# percobaan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf7679",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent = 'F03075 mousquet.dot **'\n",
    "child = 'F03726-0 grabat.dot'\n",
    "#parent = 'F03606-0 orpaillage.dot **'\n",
    "#child = 'F03842-00 bluter.dot'\n",
    "create_graph_for_missing_lexemes(parent, child, missing_folder, parent.split()[0] + '_' + child.split()[0] + '.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_missing_lexeme('grabat_Nm', 'grabataire_Adj', 'mousquet_Nm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca89c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
